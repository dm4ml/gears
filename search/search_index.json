{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Gears","text":"<p>Gears is a lightweight tool for writing control flow with LLMs with full control over your prompts. It allows you to build complex chains of actions and conditions, and execute them in a single call.</p>"},{"location":"#why-gears","title":"Why Gears?","text":"<p>Gears is so minimal; it is simply a wrapper around an LLM API call that:</p> <ul> <li>Allows you to specify your prompts as Jinja templates and inputs as Pydantic models</li> <li>Automatically handles LLM API failures with exponential backoff</li> <li>Allows you to specify control flow, based on LLM responses, in a simple, declarative way</li> </ul> <p>But the real selling point is that we are committed to not growing the codebase beyond what is necessary to support the above features. (We are not venture-backed and do not intend to be.)</p>"},{"location":"#installation","title":"Installation","text":"<p>Gears is available on PyPI, and can be installed with pip:</p> <pre><code>pip install gearsllm\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"<p>Gears has the following dependencies:</p> <ul> <li><code>python&gt;=3.9</code></li> <li><code>pydantic</code></li> <li><code>jinja2</code></li> <li><code>tenacity</code></li> <li><code>openai</code></li> </ul>"},{"location":"overview/","title":"Overview of Documentation","text":"<p>The documentation is split into the following sections:</p> <ul> <li>Primitives: The building blocks of Gears.</li> <li>Examples of Gears in action, including:</li> <li>Generating Executable SQL: A simple example of a <code>Gear</code> that generates executable SQL from a natural language query.</li> <li>Generating a Vacation Itinerary: A more complex example of a <code>Gear</code> that generates a personalized vacation itinerary.</li> <li>Writing a custom LLM: A brief primer on writing a custom LLM for use with Gears.</li> <li>Directly Manipulating Chat History: A brief primer on directly manipulating chat history.</li> </ul>"},{"location":"primitives/","title":"Primitives","text":"<p>This section describes the building blocks of Gears, including <code>Context</code>, <code>History</code>, <code>LLM</code>, and <code>Gear</code>.</p>"},{"location":"primitives/#context","title":"Context","text":"<p>A context is a Pydantic model that represents any structured information flowing through your <code>Gear</code>s. Context attributes can be accessed in prompt templates, and context objects are passed throughout <code>Gear</code> methods.</p> <p>We use Pydantic because Pydantic automatically validates data types. To create a Pydantic object, simply subclass <code>pydantic.BaseModel</code>:</p> <pre><code>from pydantic import BaseModel\nclass SomeContext(BaseModel, extra=\"allow\"): # extra=\"allow\" allows extra attributes\nsome_attribute: str # This attribute is required\nsome_other_attribute: str = None # This attribute is optional, and defaults to None\nerror: bool = False # This attribute is optional, and defaults to False\n</code></pre> <p>You can also set default values for attributes and allow extra attributes, as shown in the above example. Creating a Pydantic object is as simple as:</p> <pre><code>context = SomeContext(some_attribute=\"some value\")\n</code></pre> <p>For more information on Pydantic and extra validators, check out the Pydantic docs.</p>"},{"location":"primitives/#history","title":"History","text":"<p>The <code>History</code> class is quite straightforward---it keeps track of a list of <code>Message</code> objects, where each <code>Message</code> object has <code>role</code>, <code>content</code>, and other attributes. <code>History</code> objects are initialized with an optional <code>system_message</code>, as the <code>openai</code> docs allow for.</p> <p>To create a <code>History</code> object:</p> <pre><code>from gears import History\nhistory = History(system_message=\"You are a helpful assistant.\")\n</code></pre> <p>You should not need to add messages to a <code>History</code> object manually, as <code>gears</code> orchestrates LLMs, histories, and your control flow logic for your.</p> <p>You can print out a <code>History</code> object with:</p> <pre><code>print(history)\n</code></pre>"},{"location":"primitives/#llm","title":"LLM","text":"<p>An LLM is a class that wraps an LLM API call with exponential backoff and adds request and response data to a <code>History</code> object. We support <code>openai</code> chat models out of the box, but you can easily add your own LLMs by subclassing <code>BaseLLM</code> and following these steps. To initialize an LLM object, you should pass in the name of the chat model you want to use, as well as any other parameters you want the LLM to use (e.g., temperature):</p> <pre><code>from gears import OpenAIChat\nllm = OpenAIChat(\"gpt-3.5-turbo\", temperature=1.0)\n</code></pre> <p>You will not be calling any methods on the LLM object directly---instead, you will pass the LLM object to a <code>Gear</code> constructor, which will handle the LLM API call.</p>"},{"location":"primitives/#gear","title":"Gear","text":"<p>Your control flow will live within a <code>Gear</code> object. To create a gear, you must subclass <code>Gear</code> and implement the following methods:</p> <ul> <li><code>template</code>: A method that returns a jinja-formatted prompt template, using attributes from a context object.</li> <li><code>transform</code>: A method that transforms the response from the LLM and initial context into a new context object.</li> <li><code>switch</code>: A method that returns the next <code>Gear</code> to run, or <code>None</code> if the workflow should end.</li> </ul> <p>Here's an example of a recursive <code>Gear</code> that asks a user to write a story, and then asks them to write another story if the first story is too long:</p> <pre><code>from gears import Gear\nclass ExampleGear(Gear):\ndef template(self, context: SomeContext):\nif context.error:\nreturn \"That story was too long! Keep your story under 100 characters.\"\nelse:\nreturn \"Write a story about {{ context.some_attribute }}.\"\ndef transform(self, response: dict, context: SomeContext):\nreply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n# Suppose we only want short stories, &lt; 100 characters\nif len(reply) &gt; 100:\nreturn SomeContext(some_attribute=context.some_attribute, error=True)\nelse:\nreturn SomeContext(some_attribute=context.some_attribute, some_other_attribute=reply, error=False)\ndef switch(self, context: SomeContext): # (1)!\nif context.error:\nreturn ExampleGear(model=llm) # (2)!\nelse:\nreturn None # (3)!\n</code></pre> <ol> <li>The <code>switch</code> method is optional. If <code>switch</code> is not implemented or returns <code>None</code>, the workflow will end.</li> <li>We want to reprompt for a story if the story is too long</li> <li>If the story is not too long, we want to end the workflow</li> </ol> <p>The way to use a <code>Gear</code> is to initialize it with an LLM object, and then call the <code>run</code> method with an initial context object and history. The <code>run</code> lifecycle is as follows:</p> <ol> <li><code>run</code> calls <code>template</code> to get a prompt template, passing in the context as a Jinja variable</li> <li><code>run</code> calls the LLM API with the prompt template and the history</li> <li><code>run</code> calls <code>transform</code> with the response from the LLM and the context object. <code>transform</code> returns a new context object. If <code>transform</code> doesn't return a Pydantic object, <code>gears</code> will throw an error.</li> <li><code>run</code> calls <code>switch</code> with the new context object. If <code>switch</code> returns <code>None</code>, the workflow will end. Otherwise, the returned <code>Gear</code> will be run with the new context and history.</li> </ol> <p>Here's a full example of running a <code>Gear</code>:</p> <pre><code>async def main():\ncontext = SomeContext(some_attribute=\"a clown\")\nhistory = History(system_message=\"You are a helpful assistant.\")\nllm = OpenAIChat(\"gpt-3.5-turbo\", temperature=1.0)\ngear = ExampleGear(model=llm) # error is False by default\nfinal_context = await gear.run(context=context, history=history) # (1)!\nstory = final_context.some_other_attribute\nprint(story)\n</code></pre> <ol> <li>Runs until <code>switch</code> returns <code>None</code></li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":"<p>If you haven't already, download Gears from PyPI:</p> <pre><code>pip install gearsllm\n</code></pre> <p>This quickstart will walk you through setting up a workflow with Gears. A <code>Gear</code> is, simply, a class that wraps an LLM API call.</p>"},{"location":"quickstart/#configure-your-llm","title":"Configure your LLM","text":"<p>At the moment, Gears only supports chat-based models from <code>openai</code> and Azure <code>openai</code>. If you are using plain <code>openai</code>, simply configure your API key in your environment as described in their Python client library docs. If you are using Azure <code>openai</code>, good luck finding useful documentation---this is the best I've found.</p>"},{"location":"quickstart/#create-a-context","title":"Create a Context","text":"<p>A context is, simply, a Pydantic model that represents any structured information flowing through your <code>Gear</code>s. For example, if you are building a <code>Gear</code> that takes a user's name and returns a greeting, you might define a context like this:</p> <pre><code>from pydantic import BaseModel\nclass GreetingContext(BaseModel):\nname: str\ngreeting: str = None # This will be set by the LLM flow\n</code></pre>"},{"location":"quickstart/#connect-to-llm","title":"Connect to LLM","text":"<p>We'll use <code>openai</code>'s <code>gpt-3.5-turbo</code> for this example:</p> <pre><code>from gears import OpenAIChat\nllm = OpenAIChat(\"gpt-3.5-turbo\", temperature=1.0)\n</code></pre>"},{"location":"quickstart/#create-a-gear","title":"Create a Gear","text":"<p>A <code>Gear</code> is a class that wraps an LLM API call. A <code>Gear</code> has a jinja-formatted prompt template, <code>transform</code> method to transform the output of the LLM into a context, and <code>switch</code> method to select the next <code>Gear</code> to run. <code>Gear</code> objects are initialized with an LLM object.</p> <p>Here's a set of gears to provide a structured greeting:</p> <pre><code>from gears import Gear\nclass ComplimentGear(Gear):\ndef template(self, context: GreetingContext):\nreturn \"Write a sentence to make someone named {{ context.name }} feel good about themselves.\"\ndef transform(self, response: dict, context: GreetingContext):\nreply = response[\"choices\"][0][\"message\"][\"content\"].strip()\nreturn GreetingContext(name=context.name, greeting=reply)\ndef switch(self, context: GreetingContext):\nreturn WorldGear(context=context)\nclass WorldGear(Gear):\ndef template(self, context: GreetingContext):\nreturn \"Now write a sentence that will make {{ context.name }} feel good about humanity.\"\ndef transform(self, response: dict, context: GreetingContext):\nreply = response[\"choices\"][0][\"message\"][\"content\"].strip()\nreturn GreetingContext(name=context.name, greeting=context.greeting + \" \" + reply)\n</code></pre> <p>When running a <code>Gear</code>, Gears will automatically call the <code>transform</code> method on the response from the LLM, and then call the <code>switch</code> method to determine the next <code>Gear</code> to run. If <code>switch</code> returns <code>None</code> or is not implemented, the workflow will end.</p>"},{"location":"quickstart/#create-a-history","title":"Create a History","text":"<p>A <code>History</code> is an object that wraps messages that flow through gears, like a chat history. Simply initialize a <code>History</code> object as follows:</p> <pre><code>from gears import History\nhistory = History(system_message = \"You are an optimistic person who likes to make people feel good about themselves.\")\n</code></pre>"},{"location":"quickstart/#run-your-workflow","title":"Run your Workflow","text":"<p>To run a workflow, initialize a specific context, the top-level gear, and call the top-level gear's <code>run</code> method:</p> <pre><code>import asyncio\nasync def main():\ncontext = GreetingContext(name=\"Alice\")\ncgear = ComplimentGear(llm)\nresult_context = await cgear.run(context, history)\nprint(f\"Greeting: {result_context.greeting}\")\nprint(f\"Chat history:\\n{history}\")\nprint(f\"Cost: {history.cost}\")\nasyncio.run(main())\n</code></pre> <p>This will print:</p> <pre><code>Greeting: Alice, your kindness and empathy towards others truly sets you apart and makes the world a better place. Alice, your unwavering belief in the goodness of humanity constantly reminds us that there is still so much compassion and love in the world.\nChat history:\n[System]: You are an optimistic person who likes to make people feel good about themselves.\n[User]: Write a sentence to make someone named Alice feel good about their character.\n[Assistant]: Alice, your kindness and empathy towards others truly sets you apart and makes the world a better place.\n[User]: Now write a sentence that will make Alice feel good about the world.\n[Assistant]: Alice, your unwavering belief in the goodness of humanity constantly reminds us that there is still so much compassion and love in the world.\nCost: 0.00027749999999999997\n</code></pre> <p>That's it!</p>"},{"location":"api/gears/","title":"gears","text":""},{"location":"api/gears/#gears.Gear","title":"<code>gears.Gear</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>gears/gear.py</code> <pre><code>class Gear(ABC):\ndef __init__(\nself,\nmodel: BaseLLM,\n):\n\"\"\"Initializes a Gear with a LLM model.\n        Args:\n            model (BaseLLM): The LLM model to use. Must be an instance of a class that inherits from BaseLLM.\n        \"\"\"\nself.model = model\ndef editHistory(self, history: History, context: BaseModel) -&gt; History:\n\"\"\"Optional method to implement that edits the chat history\n        before the gear is run. Default implementation does not\n        modify the history.\n        Args:\n            history (History): Chat history so far.\n            context (BaseModel): Context of the gear.\n        Returns:\n            History: Edited chat history to be passed into the run method.\n        \"\"\"\nreturn history\nasync def run(\nself,\ncontext: BaseModel,\nhistory: History,\n**kwargs,\n) -&gt; BaseModel:\n\"\"\"Runs the gear with the given context and history. This is automatically called if executing a gear within a `switch` method; otherwise, it must be called manually for a top-level gear.\n        Args:\n            context (BaseModel): Input context for the gear. Must be a pydantic model. This is passed to the `template` method, which will construct the prompt for the LLM.\n            history (History): Chat history. This is passed to the LLM's `run` method.\n        Raises:\n            TypeError: If the `template` method does not return a string.\n            TypeError: If the `transform` method does not return a pydantic model.\n            TypeError: If the `switch` method does not return a Gear instance or None.\n        Returns:\n            BaseModel: The output context of the gear (result of `transform` method) if no gears are chained in the `switch` method; otherwise, the output context of the last gear in the chain.\n        \"\"\"\n# Construct the template with the pydantic model\ntemplate_str = self.template(context)\nif template_str is None:\n# Don't run the gear\nresponse = None\nelse:\nprompt = Template(template_str).render(context=context)\nif not isinstance(prompt, str):\nraise TypeError(\"Template must return a string\")\n# Call the model\nlogger.debug(f\"Running model with prompt: {prompt}\")\nedited_history = self.editHistory(history, context)\nresponse = await self.model.run(prompt, edited_history, **kwargs)\n# Transform the data from the response\nresponse = self.transform(response, context)\n# Verify that the structured data is a pydantic model\nif not isinstance(response, BaseModel):\nraise TypeError(\"Transform must return a pydantic model instance\")\n# Load which other gear to run, if any\ntry:\nchild = self.switch(response)\nif isinstance(child, Gear):\nreturn await child.run(response, edited_history, **kwargs)\nelif not child:\nlogger.debug(\"No child gear to run. Returning response.\")\nreturn response\nelse:\nraise TypeError(f\"Switch must return a Gear instance or None.\")\nexcept NotImplementedError:\npass\n# If there is no child, return the response\nreturn response\n@abstractmethod\ndef template(self, context: BaseModel) -&gt; str:\n\"\"\"Template for the LLM prompt. This is a jinja2 template that will be rendered with the given context.\n        Args:\n            context (BaseModel): Pydantic model instance that will be used to render the template.\n        Raises:\n            NotImplementedError: If the gear does not implement this method.\n        Returns:\n            str: Prompt template that will be rendered with the given context.\n        \"\"\"\nraise NotImplementedError(\"Gear must implement prompt template\")\n@abstractmethod\ndef transform(self, response: dict, context: BaseModel) -&gt; BaseModel:\n\"\"\"Transforms the response from the LLM into a pydantic model instance.\n        Args:\n            response (dict): Raw response from the LLM.\n            context (BaseModel): Input context for the gear. Must be a pydantic model.\n        Raises:\n            NotImplementedError: If the gear does not implement this method.\n        Returns:\n            BaseModel: New context for the gear. Must be a pydantic model instance.\n        \"\"\"\nraise NotImplementedError(\"Gear must implement transform method\")\ndef switch(self, return_context: BaseModel) -&gt; Optional[\"Gear\"]:\n\"\"\"Method that determines which gear to run next. This method is called after the `transform` method with the returned context. If this method is not implemented, then the gear will return the response from the `transform` method.\n        Args:\n            return_context (BaseModel): Output context from the `transform` method.\n        Returns:\n            Optional[Gear]: The next gear to run (an instance). If None, then the gear will return the response from the `transform` method.\n        \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.__init__","title":"<code>__init__(model: BaseLLM)</code>","text":"<p>Initializes a Gear with a LLM model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseLLM</code> <p>The LLM model to use. Must be an instance of a class that inherits from BaseLLM.</p> required Source code in <code>gears/gear.py</code> <pre><code>def __init__(\nself,\nmodel: BaseLLM,\n):\n\"\"\"Initializes a Gear with a LLM model.\n    Args:\n        model (BaseLLM): The LLM model to use. Must be an instance of a class that inherits from BaseLLM.\n    \"\"\"\nself.model = model\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.template","title":"<code>template(context: BaseModel) -&gt; str</code>  <code>abstractmethod</code>","text":"<p>Template for the LLM prompt. This is a jinja2 template that will be rendered with the given context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>BaseModel</code> <p>Pydantic model instance that will be used to render the template.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the gear does not implement this method.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Prompt template that will be rendered with the given context.</p> Source code in <code>gears/gear.py</code> <pre><code>@abstractmethod\ndef template(self, context: BaseModel) -&gt; str:\n\"\"\"Template for the LLM prompt. This is a jinja2 template that will be rendered with the given context.\n    Args:\n        context (BaseModel): Pydantic model instance that will be used to render the template.\n    Raises:\n        NotImplementedError: If the gear does not implement this method.\n    Returns:\n        str: Prompt template that will be rendered with the given context.\n    \"\"\"\nraise NotImplementedError(\"Gear must implement prompt template\")\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.editHistory","title":"<code>editHistory(history: History, context: BaseModel) -&gt; History</code>","text":"<p>Optional method to implement that edits the chat history before the gear is run. Default implementation does not modify the history.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>History</code> <p>Chat history so far.</p> required <code>context</code> <code>BaseModel</code> <p>Context of the gear.</p> required <p>Returns:</p> Name Type Description <code>History</code> <code>History</code> <p>Edited chat history to be passed into the run method.</p> Source code in <code>gears/gear.py</code> <pre><code>def editHistory(self, history: History, context: BaseModel) -&gt; History:\n\"\"\"Optional method to implement that edits the chat history\n    before the gear is run. Default implementation does not\n    modify the history.\n    Args:\n        history (History): Chat history so far.\n        context (BaseModel): Context of the gear.\n    Returns:\n        History: Edited chat history to be passed into the run method.\n    \"\"\"\nreturn history\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.transform","title":"<code>transform(response: dict, context: BaseModel) -&gt; BaseModel</code>  <code>abstractmethod</code>","text":"<p>Transforms the response from the LLM into a pydantic model instance.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict</code> <p>Raw response from the LLM.</p> required <code>context</code> <code>BaseModel</code> <p>Input context for the gear. Must be a pydantic model.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the gear does not implement this method.</p> <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>BaseModel</code> <p>New context for the gear. Must be a pydantic model instance.</p> Source code in <code>gears/gear.py</code> <pre><code>@abstractmethod\ndef transform(self, response: dict, context: BaseModel) -&gt; BaseModel:\n\"\"\"Transforms the response from the LLM into a pydantic model instance.\n    Args:\n        response (dict): Raw response from the LLM.\n        context (BaseModel): Input context for the gear. Must be a pydantic model.\n    Raises:\n        NotImplementedError: If the gear does not implement this method.\n    Returns:\n        BaseModel: New context for the gear. Must be a pydantic model instance.\n    \"\"\"\nraise NotImplementedError(\"Gear must implement transform method\")\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.switch","title":"<code>switch(return_context: BaseModel) -&gt; Optional[Gear]</code>","text":"<p>Method that determines which gear to run next. This method is called after the <code>transform</code> method with the returned context. If this method is not implemented, then the gear will return the response from the <code>transform</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>return_context</code> <code>BaseModel</code> <p>Output context from the <code>transform</code> method.</p> required <p>Returns:</p> Type Description <code>Optional[Gear]</code> <p>Optional[Gear]: The next gear to run (an instance). If None, then the gear will return the response from the <code>transform</code> method.</p> Source code in <code>gears/gear.py</code> <pre><code>def switch(self, return_context: BaseModel) -&gt; Optional[\"Gear\"]:\n\"\"\"Method that determines which gear to run next. This method is called after the `transform` method with the returned context. If this method is not implemented, then the gear will return the response from the `transform` method.\n    Args:\n        return_context (BaseModel): Output context from the `transform` method.\n    Returns:\n        Optional[Gear]: The next gear to run (an instance). If None, then the gear will return the response from the `transform` method.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.run","title":"<code>run(context: BaseModel, history: History, **kwargs: History) -&gt; BaseModel</code>  <code>async</code>","text":"<p>Runs the gear with the given context and history. This is automatically called if executing a gear within a <code>switch</code> method; otherwise, it must be called manually for a top-level gear.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>BaseModel</code> <p>Input context for the gear. Must be a pydantic model. This is passed to the <code>template</code> method, which will construct the prompt for the LLM.</p> required <code>history</code> <code>History</code> <p>Chat history. This is passed to the LLM's <code>run</code> method.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>template</code> method does not return a string.</p> <code>TypeError</code> <p>If the <code>transform</code> method does not return a pydantic model.</p> <code>TypeError</code> <p>If the <code>switch</code> method does not return a Gear instance or None.</p> <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>BaseModel</code> <p>The output context of the gear (result of <code>transform</code> method) if no gears are chained in the <code>switch</code> method; otherwise, the output context of the last gear in the chain.</p> Source code in <code>gears/gear.py</code> <pre><code>async def run(\nself,\ncontext: BaseModel,\nhistory: History,\n**kwargs,\n) -&gt; BaseModel:\n\"\"\"Runs the gear with the given context and history. This is automatically called if executing a gear within a `switch` method; otherwise, it must be called manually for a top-level gear.\n    Args:\n        context (BaseModel): Input context for the gear. Must be a pydantic model. This is passed to the `template` method, which will construct the prompt for the LLM.\n        history (History): Chat history. This is passed to the LLM's `run` method.\n    Raises:\n        TypeError: If the `template` method does not return a string.\n        TypeError: If the `transform` method does not return a pydantic model.\n        TypeError: If the `switch` method does not return a Gear instance or None.\n    Returns:\n        BaseModel: The output context of the gear (result of `transform` method) if no gears are chained in the `switch` method; otherwise, the output context of the last gear in the chain.\n    \"\"\"\n# Construct the template with the pydantic model\ntemplate_str = self.template(context)\nif template_str is None:\n# Don't run the gear\nresponse = None\nelse:\nprompt = Template(template_str).render(context=context)\nif not isinstance(prompt, str):\nraise TypeError(\"Template must return a string\")\n# Call the model\nlogger.debug(f\"Running model with prompt: {prompt}\")\nedited_history = self.editHistory(history, context)\nresponse = await self.model.run(prompt, edited_history, **kwargs)\n# Transform the data from the response\nresponse = self.transform(response, context)\n# Verify that the structured data is a pydantic model\nif not isinstance(response, BaseModel):\nraise TypeError(\"Transform must return a pydantic model instance\")\n# Load which other gear to run, if any\ntry:\nchild = self.switch(response)\nif isinstance(child, Gear):\nreturn await child.run(response, edited_history, **kwargs)\nelif not child:\nlogger.debug(\"No child gear to run. Returning response.\")\nreturn response\nelse:\nraise TypeError(f\"Switch must return a Gear instance or None.\")\nexcept NotImplementedError:\npass\n# If there is no child, return the response\nreturn response\n</code></pre>"},{"location":"api/history/","title":"history","text":""},{"location":"api/history/#gears.History","title":"<code>gears.History</code>","text":"<p>This class is a wrapper around a list of messages. It is used to keep track of the conversation history with an LLM.</p> <p>A history is a list of messages. Each message has a role and content. The role is something like \"user\" or \"system\". The content is a string.</p>"},{"location":"api/history/#gears.history.History.cost","title":"<code>cost</code>  <code>property</code>","text":""},{"location":"api/history/#gears.history.History.__init__","title":"<code>__init__(system_message: str = None, messages: List[Message] = None, cost: float = 0.0)</code>","text":"<p>Constructor for the History class.</p> <p>Parameters:</p> Name Type Description Default <code>system_message</code> <code>str</code> <p>System message for the LLM to use, if any. E.g., \"You are a helpful assistant.\" Defaults to None.</p> <code>None</code> <code>messages</code> <code>List[Message]</code> <p>List of messages to initialize the history with. Defaults to None.</p> <code>None</code> <code>cost</code> <code>float</code> <p>(float, optional): Cost of the history. Defaults to 0.0 but should be the cost of the messages passed into the constructor.</p> <code>0.0</code>"},{"location":"api/history/#gears.history.History.increment_cost","title":"<code>increment_cost(cost: float)</code>","text":"<p>Increments the cost of the history.</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>float</code> <p>Float representing the cost to increment by (in dollars)</p> required"},{"location":"api/history/#gears.history.History.add","title":"<code>add(message: Message)</code>","text":"<p>Adds a message to the history. Used in LLM run methods.</p>"},{"location":"api/history/#gears.history.History.__getitem__","title":"<code>__getitem__(index: Union[int, slice])</code>","text":"<p>Returns the message at the given index.</p>"},{"location":"api/history/#gears.history.History.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of messages in the history.</p>"},{"location":"api/history/#gears.history.History.__iter__","title":"<code>__iter__() -&gt; iter</code>","text":"<p>An iterator over the messages in the history.</p> <p>Usage: <pre><code>h = History()\nfor message in h:\nprint(message)\n</code></pre></p> <p>Returns:</p> Name Type Description <code>iter</code> <code>iter</code> <p>An iterator over the messages in the history.</p>"},{"location":"api/history/#gears.history.History.__str__","title":"<code>__str__()</code>","text":"<p>String representation of the history.</p>"},{"location":"api/history/#gears.history.Message","title":"<code>gears.history.Message</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class is a wrapper around a message. It is a building block of a history.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the message. Something like \"user\" or \"system\".</p> <code>content</code> <code>str</code> <p>The content of the message. A string.</p> Source code in <code>gears/history.py</code> <pre><code>class Message(BaseModel, extra=\"allow\"):\n\"\"\"This class is a wrapper around a message. It is a building block of a history.\n    Attributes:\n        role (str): The role of the message. Something like \"user\" or \"system\".\n        content (str): The content of the message. A string.\n    \"\"\"\nrole: str = Field(\n...,\ndescription=\"The role of the message. Something like 'user' or 'system'.\",\n)\ncontent: str = Field(..., description=\"The content of the message. A string.\")\n</code></pre>"},{"location":"api/llms/","title":"llms","text":""},{"location":"api/llms/#gears.llms.BaseLLM","title":"<code>gears.llms.BaseLLM</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>gears/llms/base.py</code> <pre><code>class BaseLLM(ABC):\n@abstractmethod\nasync def run(\nprompt: str,\nhistory: History,\nmessage_kwargs: dict = {},\n):\npass\n</code></pre>"},{"location":"api/llms/#gears.llms.base.BaseLLM.run","title":"<code>run(prompt: str, history: History, message_kwargs: dict = {})</code>  <code>abstractmethod</code> <code>async</code>","text":"Source code in <code>gears/llms/base.py</code> <pre><code>@abstractmethod\nasync def run(\nprompt: str,\nhistory: History,\nmessage_kwargs: dict = {},\n):\npass\n</code></pre>"},{"location":"api/llms/#gears.llms.OpenAIChat","title":"<code>gears.llms.OpenAIChat</code>","text":"<p>             Bases: <code>BaseLLM</code></p>"},{"location":"api/llms/#gears.llms.oai.OpenAIChat.__init__","title":"<code>__init__(model: str = 'gpt-3.5-turbo', max_retries: int = 3, **kwargs: Any)</code>","text":"<p>Creates an OpenAI chat API wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>String representing the name of the model. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo'</code> <code>max_retries</code> <code>int</code> <p>Max number of retries for calling the OpenAI API. Defaults to 3.</p> <code>3</code> <code>**kwargs</code> <code>Any</code> <p>Any additional kwargs to pass to the OpenAI chat API.</p> <code>{}</code>"},{"location":"api/llms/#gears.llms.oai.OpenAIChat.run","title":"<code>run(prompt: str, history: History, **message_kwargs: Any) -&gt; Any</code>  <code>async</code>","text":"<p>Calls the OpenAI chat API with the given prompt and chat history. Also adds the response to the chat history and increments the cost.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to send to the OpenAI chat API.</p> required <code>history</code> <code>History</code> <p>History of the chat so far.</p> required <code>**message_kwargs</code> <code>Any</code> <p>Any additional kwargs to pass to the message.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Raw response from the OpenAI chat API.</p>"},{"location":"api/llms/#gears.llms.AzureOpenAIChat","title":"<code>gears.llms.AzureOpenAIChat</code>","text":"<p>             Bases: <code>OpenAIChat</code></p>"},{"location":"api/llms/#gears.llms.oai.AzureOpenAIChat.__init__","title":"<code>__init__(model: str = 'gpt-35-turbo', deployment_id: str = 'gpt-35-turbo', max_retries: int = 3, **kwargs: Any)</code>","text":"<p>Creates an Azure OpenAI chat API wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>String representing the name of the model. Defaults to \"gpt-3.5-turbo\". Used only to determine pricing.</p> <code>'gpt-35-turbo'</code> <code>deployment_id</code> <code>str</code> <p>String representing the deployment ID of the model based on your Azure Deployment. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-35-turbo'</code> <code>max_retries</code> <code>int</code> <p>Max number of retries for calling the OpenAI API. Defaults to 3.</p> <code>3</code> <code>**kwargs</code> <code>Any</code> <p>Any additional kwargs to pass to the chat API.</p> <code>{}</code>"},{"location":"api/llms/#gears.llms.oai.AzureOpenAIChat.run","title":"<code>run(prompt: str, history: History, **message_kwargs: Any) -&gt; Any</code>  <code>async</code>","text":"<p>Calls the Azure OpenAI chat API with the given prompt and chat history. Also adds the response to the chat history and increments the cost.</p> <p>Uses the <code>deployment_id</code> to call the relevant Azure OpenAI chat API.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to send to the OpenAI chat API.</p> required <code>history</code> <code>History</code> <p>History of the chat so far.</p> required <code>**message_kwargs</code> <code>Any</code> <p>Any additional kwargs to pass to the message.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Raw response from the OpenAI chat API.</p>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#gears.utils.extract_first_json","title":"<code>gears.utils.extract_first_json(text: str) -&gt; Union[Dict, List]</code>","text":"<p>Extracts the first JSON object or array in a string representing the output of an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The string to parse.</p> required <p>Returns:</p> Type Description <code>Union[Dict, List]</code> <p>Union[Dict, List]: The first JSON object or array.</p> Source code in <code>gears/utils.py</code> <pre><code>def extract_first_json(text: str) -&gt; Union[Dict, List]:\n\"\"\"\n    Extracts the first JSON object or array in a string representing the output of an LLM.\n    Args:\n        text (str): The string to parse.\n    Returns:\n        Union[Dict, List]: The first JSON object or array.\n    \"\"\"\ntry:\nreturn next(extract_json(text))\nexcept StopIteration:\nraise StopIteration(f\"No JSON found in text: {text}\")\n</code></pre>"},{"location":"api/utils/#gears.utils.extract_json","title":"<code>gears.utils.extract_json(text, decoder = json.JSONDecoder()) -&gt; Generator[Union[Dict, List], None, None]</code>","text":"<p>Generates all JSON objects or arrays in a string representing the output of an LLM. Copied from ChatGPT.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The string to parse.</p> required <p>Returns:</p> Type Description <code>Generator[Union[Dict, List], None, None]</code> <p>Generator[Union[Dict, List], None, None]: A generator of JSON objects or arrays.</p> Source code in <code>gears/utils.py</code> <pre><code>def extract_json(\ntext, decoder=json.JSONDecoder()\n) -&gt; Generator[Union[Dict, List], None, None]:\n\"\"\"\n    Generates all JSON objects or arrays in a string representing the output of an LLM. Copied from ChatGPT.\n    Args:\n        text (str): The string to parse.\n    Returns:\n        Generator[Union[Dict, List], None, None]: A generator of JSON objects or arrays.\n    \"\"\"\npos = 0\nwhile True:\nmatch = text.find(\"{\", pos)\nmatch2 = text.find(\"[\", pos)\n# Find the earliest opening bracket\nif (match != -1 and match &lt; match2) or match2 == -1:\nstart_pos = match\nelse:\nstart_pos = match2\nif start_pos == -1:\nbreak\ntry:\nresult, index = decoder.raw_decode(text[start_pos:])\nyield result\npos = start_pos + index\nexcept json.JSONDecodeError:\npos = start_pos + 1\n</code></pre>"},{"location":"examples/advanced/","title":"Personalized Vacation Planner (Advanced Control Flow)","text":"<p>Sometimes we will want to generate a \"tree\" of calls to an LLM, where the next call depends on the output of the previous call. For example, suppose we want to generate a structured vacation itinerary for a user that includes activities that are dependent on the climate.</p>"},{"location":"examples/advanced/#0-high-level-overview","title":"0. High-Level Overview","text":"<p>In this application, we'll use LLMs to:</p> <ul> <li>Based on a home location, pick a place to travel and season to travel there</li> <li>Select 2 activities to do at the destination, including outdoor and indoor activities (weather permitting)</li> <li>Summarize the activities into a travel itinerary</li> </ul>"},{"location":"examples/advanced/#1-create-a-context","title":"1. Create a Context","text":"<p>We'll set up a Pydantic model with information we'll extract from the LLM:</p> <pre><code>from gears import Gear, History, OpenAIChat\nfrom gears.utils import extract_first_json\nfrom pydantic import BaseModel\nfrom typing import Any\nimport asyncio\nclass Suggestion(BaseModel, extra=\"allow\"):\nhome: str\ndestination: str = None\nseason: str = None\nactivities: list = None\nitinerary: str = None\n</code></pre>"},{"location":"examples/advanced/#2-create-a-gear-to-select-a-destinationseason-and-outdoorindoor-activities","title":"2. Create a Gear to Select a Destination/Season and Outdoor/Indoor Activities","text":"<p>We'll create a <code>Gear</code> that takes a home location and generates a destination and season to travel there:</p> <pre><code>class DestinationSelection(Gear):\ndef template(self, context: Suggestion):\nreturn \"Suggest a city within a 4-5 hour flight that someone who lives in {{ context.home }} can travel to for a vacation. Pick a season that is best to travel to this destination in. Output the destination and season as a JSON with keys `destination` and `season` and values equal to the destination and season, respectively.\"\ndef transform(self, response: dict, context: Suggestion):\nreply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n# Get JSON from reply and execute it\nanswer = extract_first_json(reply)\nreturn Suggestion(\nhome=context.home,\ndestination=answer[\"destination\"],\nseason=answer[\"season\"],\n)\ndef switch(self, context: Suggestion):\nreturn IndoorOrOutdoor(OpenAIChat(\"gpt-3.5-turbo\"))\nclass IndoorOrOutdoor(Gear):\ndef template(self, context: Suggestion):\nreturn \"Based on the expected weather at the destination in the {{ context.season }} season, can the person do outdoor activities? Output your answer as a JSON with key `outdoor` and value equal to `yes` or `no`, respectively.\"\ndef transform(self, response: dict, context: Suggestion):\nreply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n# Get JSON from reply and execute it\nanswer = extract_first_json(reply)\nreturn Suggestion(\nhome=context.home,\ndestination=context.destination,\nseason=context.season,\noutdoor=answer[\"outdoor\"],\n)\ndef switch(self, context: Suggestion):\nif context.outdoor == \"yes\":\nreturn OutdoorActivitySelection(OpenAIChat(\"gpt-3.5-turbo\"))\nelse:\nreturn IndoorActivitySelection(OpenAIChat(\"gpt-3.5-turbo\"))\n</code></pre>"},{"location":"examples/advanced/#3-create-gears-to-select-the-activities","title":"3. Create Gears to Select the Activities","text":"<p>Now, we'll ask the LLM to select activities to do at the destination:</p> <pre><code>class OutdoorActivitySelection(Gear):\ndef template(self, context: Suggestion):\nreturn \"Suggest a popular outdoor activity to do in the city of {{ context.destination }} during the season of {{ context.season }}.\"\ndef transform(self, response: dict, context: Suggestion):\nreply = response[\"choices\"][0][\"message\"][\"content\"].strip()\nactivities = [reply]\nreturn Suggestion(\nhome=context.home,\ndestination=context.destination,\nseason=context.season,\nactivities=activities,\noutdoor=context.outdoor,\n)\ndef switch(self, context: Suggestion):\nreturn IndoorActivitySelection(OpenAIChat(\"gpt-3.5-turbo\"))\nclass IndoorActivitySelection(Gear):\ndef template(self, context: Suggestion):\n# If there is already an indoor activity, prompt for a different one\nif context.activities:\nreturn \"Suggest a different popular indoor activity to do in the city of {{ context.destination }} during the season of {{ context.season }}.\"\nelse:\nreturn \"Suggest a popular indoor activity to do in the city of {{ context.destination }} during the season of {{ context.season }}.\"\ndef transform(self, response: dict, context: Suggestion):\nreply = response[\"choices\"][0][\"message\"][\"content\"].strip()\nactivities = (\ncontext.activities + [reply] if context.activities else [reply]\n)\nreturn Suggestion(\nhome=context.home,\ndestination=context.destination,\nseason=context.season,\nactivities=activities,\noutdoor=context.outdoor,\n)\ndef switch(self, context: Suggestion):\nif context.outdoor != \"yes\":\n# Need another indoor activity\nreturn IndoorActivitySelection(OpenAIChat(\"gpt-3.5-turbo\"))\nelse:\n# Already have 2 activities. Go to summary\nreturn SummarizeItinerary(OpenAIChat(\"gpt-3.5-turbo\"))\n</code></pre> <p>Note that we have a special case in the <code>IndoorActivitySelection</code> gear: if the LLM recommended not to do outdoor activities at all, then we ask for 2 indoor activities. Otherwise, we ask for 1 indoor activity and 1 outdoor activity.</p>"},{"location":"examples/advanced/#4-create-a-gear-to-summarize-the-itinerary","title":"4. Create a Gear to Summarize the Itinerary","text":"<p>Finally, we'll ask the LLM to summarize the itinerary:</p> <pre><code>class SummarizeItinerary(Gear):\ndef template(self, context: Suggestion):\nreturn \"Summarize your suggested activities: {{ context.activities }} into a short personalized vacation itinerary for someone who lives in {{ context.home }} to travel to {{ context.destination }} during the {{ context.season }} season.\"\ndef transform(self, response: dict, context: Suggestion):\nreply = response[\"choices\"][0][\"message\"][\"content\"].strip()\nreturn Suggestion(\nhome=context.home,\ndestination=context.destination,\nseason=context.season,\nactivities=context.activities,\noutdoor=context.outdoor,\nitinerary=reply,\n)\n</code></pre> <p>The <code>switch</code> method is not implemented, so the workflow will end after this gear.</p>"},{"location":"examples/advanced/#5-run-the-workflow","title":"5. Run the Workflow","text":"<p>We can run the workflow for a user living in San Francisco as follows:</p> <pre><code>async def main():\ncontext = Suggestion(\nhome=\"San Francisco, CA\",\n)\nhistory = History(system_message=\"You are a professional travel agent.\")\nllm = OpenAIChat(\"gpt-3.5-turbo\")\ncontext = await DestinationSelection(llm).run(context, history)\nprint(f\"Resulting itinerary: {context.itinerary}\")\nprint(f\"Cost of using the LLM: {history.cost}\")\n# (1)!\nif __name__ == \"__main__\":\nasyncio.run(main())\n</code></pre> <ol> <li>You can also print the history to see the full conversation, i.e., <code>print(history)</code></li> </ol> <p>The output should look something like this:</p> <pre><code>Resulting itinerary: Here's a personalized vacation itinerary for your trip to New York City, NY during the Autumn season:\n\nDay 1:\n- Arrive in New York City and settle into your accommodation.\n- Take a stroll through Central Park and enjoy the beautiful autumn foliage.\n\nDay 2:\n- Explore the iconic landmarks of New York City, such as Times Square, Empire State Building, and Statue of Liberty.\n- In the evening, indulge in a fantastic Broadway show.\n\nDay 3:\n- Visit renowned museums like the Metropolitan Museum of Art or the Museum of Modern Art.\n- Discover the vibrant neighborhoods of Manhattan, such as SoHo, Greenwich Village, or Chelsea.\n\nDay 4:\n- Take a leisurely walk along the High Line, a beautiful elevated park with stunning views of the city.\n- Explore the trendy shops and boutiques in neighborhoods like Fifth Avenue or Madison Avenue.\n\nDay 5:\n- Experience the bustling local markets, such as Union Square Greenmarket or Chelsea Market.\n- Enjoy a delicious brunch and immerse yourself in the city's diverse food scene.\n\nDay 6:\n- Final day to explore any missed attractions, do some souvenir shopping, or relax in a cozy caf\u00e9.\n- Depart from New York City and return home with wonderful memories.\n\nThis itinerary combines outdoor activities like strolling in Central Park with indoor excitement like watching a Broadway show, ensuring you make the most of your trip to New York City during the beautiful autumn season.\nCost of using the LLM: 0.0022164999999999997\n</code></pre>"},{"location":"examples/customllm/","title":"Creating a Custom LLM for Gears","text":"<p>Gears supports <code>openai</code>'s chat models (as well as the Azure version) out of the box, but you can easily add your own LLMs by subclassing <code>BaseLLM</code> and following the steps below.</p> <p>To create a custom LLM, you should subclass <code>BaseLLM</code>:</p> <pre><code>class BaseLLM(ABC):\n@abstractmethod\nasync def run(\nprompt: str,\nhistory: History,\nmessage_kwargs: dict = {},\n):\npass\n</code></pre> <p>The <code>run</code> method should take in a prompt, a <code>History</code> object, and any other keyword arguments you want to pass to the LLM. The <code>run</code> method should return a response from the LLM, which will be passed to the <code>transform</code> method of the <code>Gear</code> that called the LLM. The <code>run</code> method should also update the <code>History</code> object with the request and response data, and update the history's cost.</p> <p>Here is the <code>run</code> method for <code>OpenAIChat</code>, which constructs a chat history and calls the OpenAI chat API:</p> <pre><code>async def run(\nself,\nprompt: str,\nhistory: History,\n**message_kwargs: Any,\n) -&gt; Any:\n# Construct chat history\ncurr_message = Message(role=\"user\", content=prompt, **message_kwargs)\nhistory.add(curr_message)\ntry:\nmessages = [m.model_dump() for m in history]\nexcept AttributeError:\nmessages = [m.dict() for m in history]\nrequest = {\n\"model\": self.model,\n\"messages\": messages,\n**self.api_kwargs,\n}\nresponse = await self.chat_api_call(request) # (1)!\ntry:\nreturned_message = response[\"choices\"][0][\"message\"][\"content\"]\nhistory.add(\nMessage(\nrole=response[\"choices\"][0][\"message\"][\"role\"],\ncontent=returned_message,\n)\n) # (2)!\nexcept KeyError:\nlogger.error(\nf\"Could not find message and/or role in response: {response}\"\n)\n# Increment cost\ntry:\nprompt_tokens = response[\"usage\"][\"prompt_tokens\"]\ncompletion_tokens = response[\"usage\"][\"completion_tokens\"]\ncost = (\nOPENAI_PRICING_MAP[self.model_base][\"prompt_tokens\"]\n* prompt_tokens\n+ OPENAI_PRICING_MAP[self.model_base][\"completion_tokens\"]\n* completion_tokens\n)\nhistory.increment_cost(cost) # (3)!\nexcept KeyError:\nlogger.error(\nf\"Could not find pricing for model {self.model}. Not incrementing cost.\"\n)\nreturn response\n</code></pre> <ol> <li><code>chat_api_call</code> is a helper method that calls the OpenAI chat API with the request constructed above.</li> <li><code>history.add</code> is a helper method that adds a message to the history. You must add messages to the history to be used in downstream Gears.</li> <li><code>history.increment_cost</code> is a helper method that increments the cost of the history. This way, after a workflow is run, you can see how much it cost to run the workflow.</li> </ol>"},{"location":"examples/history/","title":"Directly Manipulating Chat History","text":"<p>Gears provides a way to directly manipulate the chat history before a gear is run. This is useful for multiple reasons:</p> <ul> <li>With complex chains of gears, the cost of the conversation can quickly add up. By removing unnecessary messages from the history, you can reduce the cost of the conversation.</li> <li>You can add messages to the history that are not generated by the LLMs.</li> </ul>"},{"location":"examples/history/#example","title":"Example","text":"<p>To directly manipulate chat history, you should implement the <code>editHistory</code> method in your gear. This method takes in a <code>History</code> object and <code>pydantic</code> object and returns a <code>History</code> object. Here is an example of a gear that removes the first 2 (non-system) messages from the history:</p> <pre><code>from gears import Gear, History\nfrom pydantic import BaseModel\nclass ExampleGear(Gear):\ndef template(self, context: BaseModel) -&gt; str:\nreturn \"Hello, world!\"\ndef editHistory(self, history: History, context: BaseModel) -&gt; History:\nreturn History(system_message=history[0].content, messages=history.messages[3:], cost=history.cost) # (1)!\n</code></pre> <ol> <li>It's important to keep the <code>cost</code> of the history the same. Otherwise, the cost of the conversation will be incorrect.</li> </ol> <p>Running an instance of this gear will remove the first 2 messages from the history before the gear is run for a given context. The <code>run</code> method always calls <code>editHistory</code> before running the gear on a given context. The default implementation of <code>editHistory</code> is to return the history unchanged.</p>"},{"location":"examples/simple/","title":"Text to Executable SQL (Simple Control Flow)","text":"<p>While many natural language to SQL systems these days can generate compilable SQL, it's hard for them to generate executable SQL. For example, the following query is not executable if the <code>users</code> table does not have a <code>name</code> column:</p> <pre><code>SELECT * FROM users WHERE name = \"Alice\"\n</code></pre> <p>One of the areas where <code>gears</code> shines is the ability to validate LLM output and then dynamically decide what to do. In this tutorial, we will build a simple <code>Gear</code> that generates executable SQL from a natural language query.</p>"},{"location":"examples/simple/#0-downloads","title":"0. Downloads","text":"<p>Assuming you've already configured your <code>openai</code> keys, download the Python libraries we'll be using in this tutorial:</p> <pre><code>pip install duckdb\npip install pandas\n</code></pre> <p>We'll be using the NYC Taxicab dataset, described in this DuckDB blog post. Download it using <code>wget</code> like so:</p> <pre><code>wget https://github.com/cwida/duckdb-data/releases/download/v1.0/taxi_2019_04.parquet\n</code></pre>"},{"location":"examples/simple/#1-create-a-context","title":"1. Create a Context","text":"<p>We'll set up a <code>Context</code>:</p> <pre><code>from gears import Gear, History, OpenAIChat\nfrom gears.utils import extract_first_json\nfrom pydantic import BaseModel\nfrom typing import Any\nimport asyncio\nimport duckdb\nclass SQLContext(BaseModel):\nnlquery: str\nsql: str = None\nexception: str = None\nresult: Any = None\n# Import the NYC Taxi dataset\nduckdb.query(\n\"CREATE TABLE taxi_trips AS SELECT * FROM read_parquet('taxi_2019_04.parquet');\"\n)\ntable_statistics = duckdb.query(\"PRAGMA show_tables_expanded;\").fetchdf()\ntable_statistics_str = str(table_statistics.iloc[0].to_dict())\nsystem_prompt = (\nf\"Here's some statistics about my database:\\n{table_statistics_str}\"\n)\n</code></pre>"},{"location":"examples/simple/#2-create-a-gear","title":"2. Create a Gear","text":"<p>We'll create a <code>Gear</code> that takes a natural language query and generates executable SQL:</p> <pre><code>class SQLGear(Gear):\ndef template(self, context: SQLContext):\nif context.exception:\nreturn \"Your query failed to execute with the following error: {{ context.exception }}\\n\\nPlease try again. Output the SQL as a JSON with key `sql` and value equal to the SQL query for me to run.\"\nelse:\nreturn \"Translate the following query into SQL: {{ context.nlquery }}\\n\\nMake sure the SQL is executable. Output the SQL as a JSON with key `sql` and value equal to the SQL query for me to run.\"\ndef transform(self, response: dict, context: SQLContext):\nreply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n# Get JSON from reply and execute it\ntry:\nsql = extract_first_json(reply)[\"sql\"] # (1)!\n# Execute SQL\nresult = duckdb.query(sql).fetchall()\nreturn SQLContext(nlquery=context.nlquery, sql=sql, result=result)\nexcept Exception as e:\nreturn SQLContext(nlquery=context.nlquery, exception=str(e))\ndef switch(self, context: SQLContext):\nif context.exception is not None:\nreturn SQLGear(OpenAIChat(\"gpt-3.5-turbo\"))\nelse:\nreturn None\n</code></pre> <ol> <li><code>extract_first_json</code> is a helper function that extracts the first JSON from a string. You can write your own parser here if you'd like.</li> </ol>"},{"location":"examples/simple/#3-run-the-gear","title":"3. Run the Gear","text":"<p>We'll run the gear like so:</p> <pre><code>async def main():\ncontext = SQLContext(nlquery=\"How many trips that cost more than $10 were taken in April 2019?\")\nhistory = History(system_message=system_prompt)\nllm = OpenAIChat(\"gpt-3.5-turbo\")\ncontext = await SQLGear(llm).run(context, history)\nprint(f\"SQL query: {context.sql}\")\nprint(f\"SQL query result: {context.result}\")\nprint(f\"Cost of query: {history.cost}\")\nprint(history)\nif __name__ == \"__main__\":\nasyncio.run(main())\n</code></pre> <p>This will run the gear until it returns <code>None</code> from <code>switch</code>, denoting a valid SQL query result. The output should look something like this:</p> <pre><code>SQL query: SELECT COUNT(*) FROM taxi_trips WHERE total_amount &gt; 10 AND pickup_at &gt;= '2019-04-01' AND pickup_at &lt; '2019-05-01'\nSQL query result: [(6281980,)]\nCost of query: 0.000462\n[System]: Here's some statistics about my database:\n{'database': 'memory', 'schema': 'main', 'name': 'taxi_trips', 'column_names': ['vendor_id', 'pickup_at', 'dropoff_at', 'passenger_count', 'trip_distance', 'rate_code_id', 'store_and_fwd_flag', 'pickup_location_id', 'dropoff_location_id', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge'], 'column_types': ['VARCHAR', 'TIMESTAMP', 'TIMESTAMP', 'TINYINT', 'FLOAT', 'VARCHAR', 'VARCHAR', 'INTEGER', 'INTEGER', 'VARCHAR', 'FLOAT', 'FLOAT', 'FLOAT', 'FLOAT', 'FLOAT', 'FLOAT', 'FLOAT', 'FLOAT'], 'temporary': False}\n[User]: Translate the following query into SQL: How many trips that cost more than $10 were taken in April 2019?\n\nMake sure the SQL is executable. Output the SQL as a JSON with key `sql` and value equal to the SQL query for me to run.\n[Assistant]: {\"sql\": \"SELECT COUNT(*) FROM taxi_trips WHERE total_amount &gt; 10 AND pickup_at &gt;= '2019-04-01' AND pickup_at &lt; '2019-05-01'\"}\n</code></pre>"},{"location":"examples/simple/#extra-notes","title":"Extra Notes","text":"<p><code>gears</code> is not a full-fledged LLM guardrails library, nor does it intend to be. It just provides an interface to specify control flow, which can be used for lightweight validation of LLM outfits. If you want to use a full-fledged LLM guardrails library, I recommend you check out Guardrails, coincidentally written by another person named Shreya.</p>"}]}