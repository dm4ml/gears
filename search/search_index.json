{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Gears","text":"<p>Gears is a lightweight framework for writing control flow with LLMs with full control over your prompts. It allows you to build complex chains of actions and conditions, and execute them in a single call.</p>"},{"location":"#why-gears","title":"Why Gears?","text":"<p>Gears is so minimal; it is simply a wrapper around an LLM API call that:</p> <ul> <li>Allows you to specify your prompts as Jinja templates and inputs as Pydantic models</li> <li>Automatically handles LLM API failures with exponential backoff</li> <li>Allows you to specify control flow, based on LLM responses, in a simple, declarative way</li> </ul> <p>We will never suffer the bloat of a venture-backed open source project, and we are committed to not growing the codebase beyond what is necessary to support the above features.</p>"},{"location":"#installation","title":"Installation","text":"<p>Gears is available on PyPI, and can be installed with pip:</p> <pre><code>pip install gearsllm\n</code></pre>"},{"location":"overview/","title":"Overview of Documentation","text":"<p>The documentation is split into the following sections:</p> <ul> <li>Primitives: The building blocks of Gears, including <code>Gear</code>, <code>Context</code>, and <code>LLM</code>.</li> <li>Tutorials: Examples of Gears in action, including:<ul> <li>Generating Compilable SQL: A simple example of a <code>Gear</code> that generates compilable SQL from a natural language query.</li> <li>Generating a Vacation Itinerary: A more complex example of a <code>Gear</code> that generates a vacation itinerary given user preferences.</li> <li>Writing a custom LLM: A tutorial on writing a custom LLM for use with Gears.</li> <li>Helpful Utilities: A collection of helpful utilities for use with Gears.</li> </ul> </li> </ul>"},{"location":"primitives/","title":"Primitives","text":"<p>This section describes the building blocks of Gears, including <code>Context</code>, <code>History</code>, <code>LLM</code>, and <code>Gear</code>.</p>"},{"location":"primitives/#context","title":"Context","text":"<p>A context is a Pydantic model that represents any structured information flowing through your <code>Gear</code>s. Context attributes can be accessed in prompt templates, and context objects are passed throughout <code>Gear</code> methods.</p> <p>We use Pydantic because Pydantic automatically validates data types. To create a Pydantic object, simply subclass <code>pydantic.BaseModel</code>:</p> <pre><code>from pydantic import BaseModel\n\nclass SomeContext(BaseModel, extra=\"allow\"): # extra=\"allow\" allows extra attributes\n    some_attribute: str # This attribute is required\n    some_other_attribute: str = None # This attribute is optional, and defaults to None\n    some_default_attribute: str = \"default\" # This attribute is optional, and defaults to \"default\"\n</code></pre> <p>You can also set default values for attributes and allow extra attributes, as shown in the above example. Creating a Pydantic object is as simple as:</p> <pre><code>context = SomeContext(some_attribute=\"some value\")\n</code></pre> <p>For more information on Pydantic and extra validators, check out the Pydantic docs.</p>"},{"location":"primitives/#history","title":"History","text":"<p>The <code>History</code> class is quite straightforward---it keeps track of a list of <code>Message</code> objects, where each <code>Message</code> object has <code>role</code>, <code>content</code>, and other attributes. <code>History</code> objects are initialized with an optional <code>system_message</code>, as the <code>openai</code> docs allow for.</p> <p>To create a <code>History</code> object:</p> <pre><code>from gears import History\n\nhistory = History(system_message=\"You are a helpful assistant.\")\n</code></pre> <p>You should not need to add messages to a <code>History</code> object manually, as <code>gears</code> orchestrates LLMs, histories, and your control flow logic for your.</p> <p>You can print out a <code>History</code> object with:</p> <pre><code>print(history)\n</code></pre>"},{"location":"primitives/#llm","title":"LLM","text":"<p>An <code>LLM</code> is a class that wraps an LLM API call with exponential backoff and adds request and response data to a <code>History</code> object. We support <code>openai</code> chat models out of the box, but you can easily add your own LLMs by subclassing <code>BaseLLM</code> and following these steps. To initialize an LLM object, you should pass in the name of the chat model you want to use, as well as any other parameters you want the LLM to use (e.g., temperature):</p> <pre><code>from gears import OpenAIChat\n\nllm = OpenAIChat(\"gpt-3.5-turbo\", temperature=1.0)\n</code></pre> <p>You will not be calling any methods on the LLM object directly---instead, you will pass the LLM object to a <code>Gear</code> constructor, which will handle the LLM API call.</p>"},{"location":"primitives/#gear","title":"Gear","text":"<p>Your control flow will live within a <code>Gear</code> object. To create a gear, you must subclass <code>Gear</code> and implement the following methods:</p> <ul> <li><code>template</code>: A method that returns a jinja-formatted prompt template, using attributes from a context object.</li> <li><code>transform</code>: A method that transforms the response from the LLM and initial context into a new context object.</li> <li><code>switch</code>: A method that returns the next <code>Gear</code> to run, or <code>None</code> if the workflow should end.</li> </ul> <p>Here's an example of a recursive <code>Gear</code> that asks a user to write a story, and then asks them to write another story if the first story is too long:</p> <pre><code>from gears import Gear\n\nclass ExampleGear(Gear):\n    def __init__(self, model: OpenAIChat, error: bool = False, **kwargs): # (1)!\n        self.error = error\n        super().__init__(model, **kwargs)\n\n    def template(self):\n        if self.error:\n            return \"That story was too long! Keep your story under 100 characters.\"\n        else:\n            return \"Write a story about {{ some_attribute }}.\"\n\n    def transform(self, response: dict, context: SomeContext):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        # Suppose we only want short stories, &lt; 100 characters\n        if len(reply) &gt; 100:\n            return SomeContext(some_attribute=context.some_attribute, error=True)\n        else:\n            return SomeContext(some_attribute=context.some_attribute, some_other_attribute=reply, error=False)\n\n    def switch(self, context: SomeContext): # (2)!\n        if context.error:\n            return ExampleGear(error=True, model=llm) # (3)!\n        else:\n            return None # (4)!\n</code></pre> <ol> <li>Constructors are optional. The default constructor takes in a <code>BaseLLM</code> object, i.e., the LLM.</li> <li>The <code>switch</code> method is optional. If <code>switch</code> is not implemented or returns <code>None</code>, the workflow will end.</li> <li>We want to reprompt for a story if the story is too long</li> <li>If the story is not too long, we want to end the workflow</li> </ol> <p>The way to use a <code>Gear</code> is to initialize it with an LLM object, and then call the <code>run</code> method with an initial context object and history. The <code>run</code> lifecycle is as follows:</p> <ol> <li><code>run</code> calls <code>template</code> to get a prompt template, passing in the context object attributes as Jinja variables</li> <li><code>run</code> calls the LLM API with the prompt template and the history</li> <li><code>run</code> calls <code>transform</code> with the response from the LLM and the context object. <code>transform</code> returns a new context object. If <code>transform</code> doesn't return a Pydantic object, <code>gears</code> will throw an error.</li> <li><code>run</code> calls <code>switch</code> with the new context object. If <code>switch</code> returns <code>None</code>, the workflow will end. Otherwise, the returned <code>Gear</code> will be run with the new context and history.</li> </ol> <p>Here's a full example of running a <code>Gear</code>:</p> <pre><code>async def main():\n    context = SomeContext(some_attribute=\"a clown\")\n    history = History(system_message=\"You are a helpful assistant.\")\n    llm = OpenAIChat(\"gpt-3.5-turbo\", temperature=1.0)\n\n    gear = ExampleGear(model=llm) # error is False by default\n    final_context = await gear.run(context=context, history=history) # (1)!\n    story = final_context.some_other_attribute\n\n    print(story)\n</code></pre> <ol> <li>Runs until <code>switch</code> returns <code>None</code></li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":"<p>If you haven't already, download Gears from PyPI:</p> <pre><code>pip install gearsllm\n</code></pre> <p>This quickstart will walk you through setting up a workflow with Gears. A <code>Gear</code> is, simply, a class that wraps an LLM API call.</p>"},{"location":"quickstart/#configure-your-llm","title":"Configure your LLM","text":"<p>At the moment, Gears only supports chat-based models from <code>openai</code> and Azure <code>openai</code>. If you are using plain <code>openai</code>, simply configure your API key in your environment as described in their Python client library docs. If you are using Azure <code>openai</code>, good luck finding useful documentation---this is the best I've found.</p>"},{"location":"quickstart/#create-a-context","title":"Create a Context","text":"<p>A context is, simply, a Pydantic model that represents any structured information flowing through your <code>Gear</code>s. For example, if you are building a <code>Gear</code> that takes a user's name and returns a greeting, you might define a context like this:</p> <pre><code>from pydantic import BaseModel\n\nclass GreetingContext(BaseModel):\n    name: str\n    greeting: str = None # This will be set by the LLM flow\n</code></pre>"},{"location":"quickstart/#connect-to-llm","title":"Connect to LLM","text":"<p>We'll use <code>openai</code>'s <code>gpt-3.5-turbo</code> for this example:</p> <pre><code>from gears import OpenAIChat\n\nllm = OpenAIChat(\"gpt-3.5-turbo\", temperature=1.0)\n</code></pre>"},{"location":"quickstart/#create-a-gear","title":"Create a Gear","text":"<p>A <code>Gear</code> is a class that wraps an LLM API call. A <code>Gear</code> has a jinja-formatted prompt template, <code>transform</code> method to transform the output of the LLM into a context, and <code>switch</code> method to select the next <code>Gear</code> to run. <code>Gear</code> objects are initialized with an LLM object.</p> <p>Here's a set of gears to provide a structured greeting:</p> <pre><code>from gears import Gear\n\nclass ComplimentGear(Gear):\n    def template(self):\n        return \"Write a sentence to make someone named {{ name }} feel good about their character.\"\n\n    def transform(self, response: dict, context: GreetingContext):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        return GreetingContext(name=context.name, greeting=reply)\n\n    def switch(self, context: GreetingContext):\n        return HumanityGear(context=context)\n\nclass HumanityGear(Gear):\n    def template(self):\n        return \"Now write a sentence that will make {{ name }} feel good about humanity.\"\n\n    def transform(self, response: dict, context: GreetingContext):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        return GreetingContext(name=context.name, greeting=context.greeting + \" \" + reply)\n</code></pre> <p>When running a <code>Gear</code>, Gears will automatically call the <code>transform</code> method on the response from the LLM, and then call the <code>switch</code> method to determine the next <code>Gear</code> to run. If <code>switch</code> returns <code>None</code> or is not implemented, the workflow will end.</p>"},{"location":"quickstart/#create-a-history","title":"Create a History","text":"<p>A <code>History</code> is an object that wraps messages that flow through gears, like a chat history. Simply initialize a <code>History</code> object as follows:</p> <pre><code>from gears import History\n\nhistory = History(system_message = \"You are an optimistic person who likes to make people feel good about themselves.\")\n</code></pre>"},{"location":"quickstart/#run-your-workflow","title":"Run your Workflow","text":"<p>To run a workflow, initialize a specific context, the top-level gear, and call the top-level gear's <code>run</code> method:</p> <pre><code>import asyncio\n\nasync def main():\n    context = GreetingContext(name=\"Alice\")\n    cgear = ComplimentGear(llm)\n    result_context = await cgear.run(context, history)\n    print(f\"Greeting: {result_context.greeting}\")\n    print(f\"Chat history:\\n{history}\")\n    print(f\"Cost: {history.cost}\")\n\nasyncio.run(main())\n</code></pre> <p>This will print:</p> <pre><code>Greeting: Alice, your kindness and empathy towards others truly sets you apart and makes the world a better place. Alice, your unwavering belief in the goodness of humanity constantly reminds us that there is still so much compassion and love in the world.\nChat history:\n[System]: You are an optimistic person who likes to make people feel good about themselves.\n[User]: Write a sentence to make someone named Alice feel good about their character.\n[Assistant]: Alice, your kindness and empathy towards others truly sets you apart and makes the world a better place.\n[User]: Now write a sentence that will make Alice feel good about humanity.\n[Assistant]: Alice, your unwavering belief in the goodness of humanity constantly reminds us that there is still so much compassion and love in the world.\nCost: 0.00027749999999999997\n</code></pre> <p>That's it!</p>"},{"location":"api/gears/","title":"gears","text":""},{"location":"api/gears/#gears.Gear","title":"<code>gears.Gear</code>","text":"Source code in <code>gears/gear.py</code> <pre><code>class Gear:\n    def __init__(\n        self,\n        model: BaseLLM,\n    ):\n        self.model = model\n        self.template = self.template()\n\n    async def run(\n        self,\n        data: BaseModel,\n        history: History,\n        **kwargs,\n    ):\n        # Construct the template with the pydantic model\n        try:\n            items = data.model_dump()\n        except AttributeError:\n            items = data.dict()\n\n        prompt = Template(self.template).render(items)\n\n        # Call the model\n        logger.info(f\"Running model with prompt: {prompt}\")\n        response = await self.model.run(prompt, history, **kwargs)\n\n        # Transform the data from the response\n        try:\n            response = self.transform(response, data)\n            # Verify that the structured data is a pydantic model\n            if not isinstance(response, BaseModel):\n                raise TypeError(\n                    \"Transform must return a pydantic model instance\"\n                )\n        except NotImplementedError:\n            pass\n\n        # Load which other gear to run, if any\n        try:\n            child = self.switch(response)\n            if isinstance(child, Gear):\n                return await child.run(response, history, **kwargs)\n            elif not child:\n                logger.info(\"No child gear to run. Returning response.\")\n                return response\n            else:\n                raise TypeError(f\"Switch must return a Gear instance or None.\")\n\n        except NotImplementedError:\n            pass\n\n        # If there is no child, return the response\n        return response\n\n    def template(self) -&gt; str:\n        raise NotImplementedError(\"Gear must implement prompt template\")\n\n    def transform(self, response: dict, **kwargs) -&gt; BaseModel:\n        raise NotImplementedError\n\n    def switch(self, response: BaseModel, **kwargs) -&gt; \"Gear\":\n        raise NotImplementedError\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.__init__","title":"<code>__init__(model: BaseLLM)</code>","text":"Source code in <code>gears/gear.py</code> <pre><code>def __init__(\n    self,\n    model: BaseLLM,\n):\n    self.model = model\n    self.template = self.template()\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.template","title":"<code>template() -&gt; str</code>","text":"Source code in <code>gears/gear.py</code> <pre><code>def template(self) -&gt; str:\n    raise NotImplementedError(\"Gear must implement prompt template\")\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.transform","title":"<code>transform(response: dict, **kwargs: dict) -&gt; BaseModel</code>","text":"Source code in <code>gears/gear.py</code> <pre><code>def transform(self, response: dict, **kwargs) -&gt; BaseModel:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.switch","title":"<code>switch(response: BaseModel, **kwargs: BaseModel) -&gt; Gear</code>","text":"Source code in <code>gears/gear.py</code> <pre><code>def switch(self, response: BaseModel, **kwargs) -&gt; \"Gear\":\n    raise NotImplementedError\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.run","title":"<code>run(data: BaseModel, history: History, **kwargs: History)</code>  <code>async</code>","text":"Source code in <code>gears/gear.py</code> <pre><code>async def run(\n    self,\n    data: BaseModel,\n    history: History,\n    **kwargs,\n):\n    # Construct the template with the pydantic model\n    try:\n        items = data.model_dump()\n    except AttributeError:\n        items = data.dict()\n\n    prompt = Template(self.template).render(items)\n\n    # Call the model\n    logger.info(f\"Running model with prompt: {prompt}\")\n    response = await self.model.run(prompt, history, **kwargs)\n\n    # Transform the data from the response\n    try:\n        response = self.transform(response, data)\n        # Verify that the structured data is a pydantic model\n        if not isinstance(response, BaseModel):\n            raise TypeError(\n                \"Transform must return a pydantic model instance\"\n            )\n    except NotImplementedError:\n        pass\n\n    # Load which other gear to run, if any\n    try:\n        child = self.switch(response)\n        if isinstance(child, Gear):\n            return await child.run(response, history, **kwargs)\n        elif not child:\n            logger.info(\"No child gear to run. Returning response.\")\n            return response\n        else:\n            raise TypeError(f\"Switch must return a Gear instance or None.\")\n\n    except NotImplementedError:\n        pass\n\n    # If there is no child, return the response\n    return response\n</code></pre>"},{"location":"api/history/","title":"history","text":""},{"location":"api/history/#gears.History","title":"<code>gears.History</code>","text":"<p>This class is a wrapper around a list of messages. It is used to keep track of the conversation history with an LLM.</p> <p>A history is a list of messages. Each message has a role and content. The role is something like \"user\" or \"system\". The content is a string.</p>"},{"location":"api/history/#gears.history.History.cost","title":"<code>cost</code>  <code>property</code>","text":""},{"location":"api/history/#gears.history.History.__init__","title":"<code>__init__(system_message: str = None)</code>","text":"<p>Constructor for the History class.</p> <p>Parameters:</p> Name Type Description Default <code>system_message</code> <code>str</code> <p>System message for the LLM to use, if any. E.g., \"You are a helpful assistant.\" Defaults to None.</p> <code>None</code>"},{"location":"api/history/#gears.history.History.increment_cost","title":"<code>increment_cost(cost: float)</code>","text":"<p>Increments the cost of the history.</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>float</code> <p>Float representing the cost to increment by (in dollars)</p> required"},{"location":"api/history/#gears.history.History.add","title":"<code>add(message: Message)</code>","text":"<p>Adds a message to the history. Used in LLM run methods.</p>"},{"location":"api/history/#gears.history.History.__getitem__","title":"<code>__getitem__(index: int)</code>","text":"<p>Returns the message at the given index.</p>"},{"location":"api/history/#gears.history.History.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of messages in the history.</p>"},{"location":"api/history/#gears.history.History.__iter__","title":"<code>__iter__()</code>","text":"<p>An iterator over the messages in the history.</p> <p>Usage: <pre><code>h = History()\n\nfor message in h:\n    print(message)\n</code></pre></p> <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p>"},{"location":"api/history/#gears.history.History.__str__","title":"<code>__str__()</code>","text":"<p>String representation of the history.</p>"},{"location":"api/history/#gears.history.Message","title":"<code>gears.history.Message</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class is a wrapper around a message. It is a building block of a history.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the message. Something like \"user\" or \"system\".</p> <code>content</code> <code>str</code> <p>The content of the message. A string.</p> Source code in <code>gears/history.py</code> <pre><code>class Message(BaseModel, extra=\"allow\"):\n\"\"\"This class is a wrapper around a message. It is a building block of a history.\n\n    Attributes:\n        role (str): The role of the message. Something like \"user\" or \"system\".\n        content (str): The content of the message. A string.\n    \"\"\"\n\n    role: str = Field(\n        ...,\n        description=\"The role of the message. Something like 'user' or 'system'.\",\n    )\n    content: str = Field(\n        ..., description=\"The content of the message. A string.\"\n    )\n</code></pre>"},{"location":"examples/advanced_control_flow/","title":"Personalized Vacation Planner (Advanced Control Flow)","text":""},{"location":"examples/simple_control_flow/","title":"Text to Executable SQL (Simple Control Flow)","text":"<p>While many natural language to SQL systems these days can generate compilable SQL, it's hard for them to generate executable SQL. For example, the following query is not executable if the <code>users</code> table does not have a <code>name</code> column:</p> <pre><code>SELECT * FROM users WHERE name = \"Alice\"\n</code></pre> <p>One of the areas where <code>gears</code> shines is the ability to validate LLM output and dynamically decide what to do. In this tutorial, we will build a simple <code>Gear</code> that generates executable SQL from a natural language query.</p>"},{"location":"examples/simple_control_flow/#0-downloads","title":"0. Downloads","text":"<p>Assuming you've already configured your <code>openai</code> keys, download the Python libraries we'll be using in this tutorial:</p> <pre><code>pip install duckdb\n</code></pre> <p>We'll be using the NYC Taxicab dataset, described in this DuckDB blog post. Download it using <code>wget</code> like so:</p> <pre><code>wget https://github.com/cwida/duckdb-data/releases/download/v1.0/taxi_2019_04.parquet\n</code></pre>"},{"location":"examples/simple_control_flow/#1-create-a-context","title":"1. Create a Context","text":"<p>We'll set up a <code>Context</code>:</p> <pre><code>from gears import Gear, History, OpenAIChat\nfrom pydantic import BaseModel\nfrom typing import Any\n\nimport asyncio\nimport duckdb\n\nclass SQLContext(BaseModel):\n    nlquery: str\n    sql: str = None\n    exception: str = None\n    result: Any = None\n\ntable_statistics = duckdb.query(\"PRAGMA show_tables_expanded;\").fetchdf()\nsystem_prompt = f\"Here's some statistics about my database:\\n{table_statistics.to_string(index=False)}\"\n</code></pre>"},{"location":"examples/simple_control_flow/#2-create-a-gear","title":"2. Create a Gear","text":"<p>We'll create a <code>Gear</code> that takes a natural language query and generates executable SQL:</p> <pre><code>from gears.utils import extract_first_json\n\nclass SQLGear(Gear):\n    def __init__(self, model: OpenAIChat, error: bool = False, **kwargs):\n        self.error = error\n        super().__init__(model, **kwargs)\n\n    def template(self):\n        if self.error:\n            return \"Your query failed to execute with the following error: {{ exception }}\\n\\nPlease try again. Output the SQL as a JSON with key `sql` and value equal to the SQL query for me to run.\"\n\n        else:\n            return \"Translate the following query into SQL: {{ nlquery }}\\n\\nMake sure the SQL is executable. Output the SQL as a JSON with key `sql` and value equal to the SQL query for me to run.\"\n\n    def transform(self, response: dict, context: SQLContext):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        # Get JSON from reply and execute it\n        try:\n            sql = extract_first_json(reply, context) # (1)!\n\n            # Execute SQL\n            result = duckdb.query(sql).fetchall()\n            return SQLContext(nlquery=context.nlquery, sql=sql, result=result)\n\n        except Exception as e:\n            return SQLContext(nlquery=context.nlquery, exception=str(e))\n\n    def switch(self, context: SQLContext):\n        if context.exception is not None:\n            return SQLGear(OpenAIChat(\"gpt-3.5-turbo\"), error=True)\n        else:\n            return None\n</code></pre> <ol> <li><code>extract_first_json</code> is a helper function that extracts the first JSON from a string. You can write your own parser here if you'd like.</li> </ol>"},{"location":"examples/simple_control_flow/#3-run-the-gear","title":"3. Run the Gear","text":"<p>We'll run the gear like so:</p> <pre><code>async def main():\n    context = SQLContext(nlquery=\"How many trips were taken in April 2019?\")\n    history = History(system_message=system_prompt)\n    llm = OpenAIChat(\"gpt-3.5-turbo\")\n\n    context = await SQLGear(llm).run(context, history)\n    print(f\"SQL query: {context.sql}\")\n    print(f\"SQL query result: {context.result}\")\n    print(f\"Cost of query: {history.cost}\")\n    print(history)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This will run the gear until it returns <code>None</code> from <code>switch</code>, denoting a valid SQL query result. The output should look something like this:</p> <pre><code>TODO\n</code></pre>"},{"location":"examples/simple_control_flow/#extra-notes","title":"Extra Notes","text":"<p><code>gears</code> is not a full-fledged LLM guardrails library, nor does it intend to be. It is just a lightweight way to add some validation criteria to your LLMs by using control flow. If you want to use a full-fledged LLM guardrails library, I recommend you check out Guardrails, coincidentally written by another person named Shreya.</p>"}]}