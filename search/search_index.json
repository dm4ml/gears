{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Gears","text":"<p>Gears is a lightweight framework for writing control flow with LLMs with full control over your prompts. It allows you to build complex chains of actions and conditions, and execute them in a single call.</p>"},{"location":"#why-gears","title":"Why Gears?","text":"<p>Gears is so minimal; it is simply a wrapper around an LLM API call that:</p> <ul> <li>Allows you to specify your prompts as Jinja templates and inputs as Pydantic models</li> <li>Automatically handles LLM API failures with exponential backoff</li> <li>Allows you to specify control flow, based on LLM responses, in a simple, declarative way</li> </ul> <p>But the real selling point is that we are committed to not growing the codebase beyond what is necessary to support the above features. (We are not venture-backed and do not intend to be.)</p>"},{"location":"#installation","title":"Installation","text":"<p>Gears is available on PyPI, and can be installed with pip:</p> <pre><code>pip install gearsllm\n</code></pre>"},{"location":"overview/","title":"Overview of Documentation","text":"<p>The documentation is split into the following sections:</p> <ul> <li>Primitives: The building blocks of Gears, including <code>Gear</code>, <code>Context</code>, and <code>LLM</code>.</li> <li>Examples of Gears in action, including:<ul> <li>Generating Executable SQL: A simple example of a <code>Gear</code> that generates executable SQL from a natural language query.</li> <li>Generating a Vacation Itinerary: A more complex example of a <code>Gear</code> that generates a personalized vacation itinerary.</li> <li>Writing a custom LLM: A brief primer on writing a custom LLM for use with Gears.</li> </ul> </li> </ul>"},{"location":"primitives/","title":"Primitives","text":"<p>This section describes the building blocks of Gears, including <code>Context</code>, <code>History</code>, <code>LLM</code>, and <code>Gear</code>.</p>"},{"location":"primitives/#context","title":"Context","text":"<p>A context is a Pydantic model that represents any structured information flowing through your <code>Gear</code>s. Context attributes can be accessed in prompt templates, and context objects are passed throughout <code>Gear</code> methods.</p> <p>We use Pydantic because Pydantic automatically validates data types. To create a Pydantic object, simply subclass <code>pydantic.BaseModel</code>:</p> <pre><code>from pydantic import BaseModel\n\nclass SomeContext(BaseModel, extra=\"allow\"): # extra=\"allow\" allows extra attributes\n    some_attribute: str # This attribute is required\n    some_other_attribute: str = None # This attribute is optional, and defaults to None\n    error: bool = False # This attribute is optional, and defaults to False\n</code></pre> <p>You can also set default values for attributes and allow extra attributes, as shown in the above example. Creating a Pydantic object is as simple as:</p> <pre><code>context = SomeContext(some_attribute=\"some value\")\n</code></pre> <p>For more information on Pydantic and extra validators, check out the Pydantic docs.</p>"},{"location":"primitives/#history","title":"History","text":"<p>The <code>History</code> class is quite straightforward---it keeps track of a list of <code>Message</code> objects, where each <code>Message</code> object has <code>role</code>, <code>content</code>, and other attributes. <code>History</code> objects are initialized with an optional <code>system_message</code>, as the <code>openai</code> docs allow for.</p> <p>To create a <code>History</code> object:</p> <pre><code>from gears import History\n\nhistory = History(system_message=\"You are a helpful assistant.\")\n</code></pre> <p>You should not need to add messages to a <code>History</code> object manually, as <code>gears</code> orchestrates LLMs, histories, and your control flow logic for your.</p> <p>You can print out a <code>History</code> object with:</p> <pre><code>print(history)\n</code></pre>"},{"location":"primitives/#llm","title":"LLM","text":"<p>An <code>LLM</code> is a class that wraps an LLM API call with exponential backoff and adds request and response data to a <code>History</code> object. We support <code>openai</code> chat models out of the box, but you can easily add your own LLMs by subclassing <code>BaseLLM</code> and following these steps. To initialize an LLM object, you should pass in the name of the chat model you want to use, as well as any other parameters you want the LLM to use (e.g., temperature):</p> <pre><code>from gears import OpenAIChat\n\nllm = OpenAIChat(\"gpt-3.5-turbo\", temperature=1.0)\n</code></pre> <p>You will not be calling any methods on the LLM object directly---instead, you will pass the LLM object to a <code>Gear</code> constructor, which will handle the LLM API call.</p>"},{"location":"primitives/#gear","title":"Gear","text":"<p>Your control flow will live within a <code>Gear</code> object. To create a gear, you must subclass <code>Gear</code> and implement the following methods:</p> <ul> <li><code>template</code>: A method that returns a jinja-formatted prompt template, using attributes from a context object.</li> <li><code>transform</code>: A method that transforms the response from the LLM and initial context into a new context object.</li> <li><code>switch</code>: A method that returns the next <code>Gear</code> to run, or <code>None</code> if the workflow should end.</li> </ul> <p>Here's an example of a recursive <code>Gear</code> that asks a user to write a story, and then asks them to write another story if the first story is too long:</p> <pre><code>from gears import Gear\n\nclass ExampleGear(Gear):\n    def template(self, context: SomeContext):\n        if context.error:\n            return \"That story was too long! Keep your story under 100 characters.\"\n        else:\n            return \"Write a story about {{ context.some_attribute }}.\"\n\n    def transform(self, response: dict, context: SomeContext):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        # Suppose we only want short stories, &lt; 100 characters\n        if len(reply) &gt; 100:\n            return SomeContext(some_attribute=context.some_attribute, error=True)\n        else:\n            return SomeContext(some_attribute=context.some_attribute, some_other_attribute=reply, error=False)\n\n    def switch(self, context: SomeContext): # (1)!\n        if context.error:\n            return ExampleGear(model=llm) # (2)!\n        else:\n            return None # (3)!\n</code></pre> <ol> <li>The <code>switch</code> method is optional. If <code>switch</code> is not implemented or returns <code>None</code>, the workflow will end.</li> <li>We want to reprompt for a story if the story is too long</li> <li>If the story is not too long, we want to end the workflow</li> </ol> <p>The way to use a <code>Gear</code> is to initialize it with an LLM object, and then call the <code>run</code> method with an initial context object and history. The <code>run</code> lifecycle is as follows:</p> <ol> <li><code>run</code> calls <code>template</code> to get a prompt template, passing in the context as a Jinja variable</li> <li><code>run</code> calls the LLM API with the prompt template and the history</li> <li><code>run</code> calls <code>transform</code> with the response from the LLM and the context object. <code>transform</code> returns a new context object. If <code>transform</code> doesn't return a Pydantic object, <code>gears</code> will throw an error.</li> <li><code>run</code> calls <code>switch</code> with the new context object. If <code>switch</code> returns <code>None</code>, the workflow will end. Otherwise, the returned <code>Gear</code> will be run with the new context and history.</li> </ol> <p>Here's a full example of running a <code>Gear</code>:</p> <pre><code>async def main():\n    context = SomeContext(some_attribute=\"a clown\")\n    history = History(system_message=\"You are a helpful assistant.\")\n    llm = OpenAIChat(\"gpt-3.5-turbo\", temperature=1.0)\n\n    gear = ExampleGear(model=llm) # error is False by default\n    final_context = await gear.run(context=context, history=history) # (1)!\n    story = final_context.some_other_attribute\n\n    print(story)\n</code></pre> <ol> <li>Runs until <code>switch</code> returns <code>None</code></li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":"<p>If you haven't already, download Gears from PyPI:</p> <pre><code>pip install gearsllm\n</code></pre> <p>This quickstart will walk you through setting up a workflow with Gears. A <code>Gear</code> is, simply, a class that wraps an LLM API call.</p>"},{"location":"quickstart/#configure-your-llm","title":"Configure your LLM","text":"<p>At the moment, Gears only supports chat-based models from <code>openai</code> and Azure <code>openai</code>. If you are using plain <code>openai</code>, simply configure your API key in your environment as described in their Python client library docs. If you are using Azure <code>openai</code>, good luck finding useful documentation---this is the best I've found.</p>"},{"location":"quickstart/#create-a-context","title":"Create a Context","text":"<p>A context is, simply, a Pydantic model that represents any structured information flowing through your <code>Gear</code>s. For example, if you are building a <code>Gear</code> that takes a user's name and returns a greeting, you might define a context like this:</p> <pre><code>from pydantic import BaseModel\n\nclass GreetingContext(BaseModel):\n    name: str\n    greeting: str = None # This will be set by the LLM flow\n</code></pre>"},{"location":"quickstart/#connect-to-llm","title":"Connect to LLM","text":"<p>We'll use <code>openai</code>'s <code>gpt-3.5-turbo</code> for this example:</p> <pre><code>from gears import OpenAIChat\n\nllm = OpenAIChat(\"gpt-3.5-turbo\", temperature=1.0)\n</code></pre>"},{"location":"quickstart/#create-a-gear","title":"Create a Gear","text":"<p>A <code>Gear</code> is a class that wraps an LLM API call. A <code>Gear</code> has a jinja-formatted prompt template, <code>transform</code> method to transform the output of the LLM into a context, and <code>switch</code> method to select the next <code>Gear</code> to run. <code>Gear</code> objects are initialized with an LLM object.</p> <p>Here's a set of gears to provide a structured greeting:</p> <pre><code>from gears import Gear\n\nclass ComplimentGear(Gear):\n    def template(self, context: GreetingContext):\n        return \"Write a sentence to make someone named {{ context.name }} feel good about their character.\"\n\n    def transform(self, response: dict, context: GreetingContext):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        return GreetingContext(name=context.name, greeting=reply)\n\n    def switch(self, context: GreetingContext):\n        return HumanityGear(context=context)\n\nclass HumanityGear(Gear):\n    def template(self, context: GreetingContext):\n        return \"Now write a sentence that will make {{ context.name }} feel good about humanity.\"\n\n    def transform(self, response: dict, context: GreetingContext):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        return GreetingContext(name=context.name, greeting=context.greeting + \" \" + reply)\n</code></pre> <p>When running a <code>Gear</code>, Gears will automatically call the <code>transform</code> method on the response from the LLM, and then call the <code>switch</code> method to determine the next <code>Gear</code> to run. If <code>switch</code> returns <code>None</code> or is not implemented, the workflow will end.</p>"},{"location":"quickstart/#create-a-history","title":"Create a History","text":"<p>A <code>History</code> is an object that wraps messages that flow through gears, like a chat history. Simply initialize a <code>History</code> object as follows:</p> <pre><code>from gears import History\n\nhistory = History(system_message = \"You are an optimistic person who likes to make people feel good about themselves.\")\n</code></pre>"},{"location":"quickstart/#run-your-workflow","title":"Run your Workflow","text":"<p>To run a workflow, initialize a specific context, the top-level gear, and call the top-level gear's <code>run</code> method:</p> <pre><code>import asyncio\n\nasync def main():\n    context = GreetingContext(name=\"Alice\")\n    cgear = ComplimentGear(llm)\n    result_context = await cgear.run(context, history)\n    print(f\"Greeting: {result_context.greeting}\")\n    print(f\"Chat history:\\n{history}\")\n    print(f\"Cost: {history.cost}\")\n\nasyncio.run(main())\n</code></pre> <p>This will print:</p> <pre><code>Greeting: Alice, your kindness and empathy towards others truly sets you apart and makes the world a better place. Alice, your unwavering belief in the goodness of humanity constantly reminds us that there is still so much compassion and love in the world.\nChat history:\n[System]: You are an optimistic person who likes to make people feel good about themselves.\n[User]: Write a sentence to make someone named Alice feel good about their character.\n[Assistant]: Alice, your kindness and empathy towards others truly sets you apart and makes the world a better place.\n[User]: Now write a sentence that will make Alice feel good about humanity.\n[Assistant]: Alice, your unwavering belief in the goodness of humanity constantly reminds us that there is still so much compassion and love in the world.\nCost: 0.00027749999999999997\n</code></pre> <p>That's it!</p>"},{"location":"api/gears/","title":"gears","text":""},{"location":"api/gears/#gears.Gear","title":"<code>gears.Gear</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>gears/gear.py</code> <pre><code>class Gear(ABC):\n    def __init__(\n        self,\n        model: BaseLLM,\n    ):\n\"\"\"Initializes a Gear with a LLM model.\n\n        Args:\n            model (BaseLLM): The LLM model to use. Must be an instance of a class that inherits from BaseLLM.\n        \"\"\"\n        self.model = model\n\n    async def run(\n        self,\n        context: BaseModel,\n        history: History,\n        **kwargs,\n    ) -&gt; BaseModel:\n\"\"\"Runs the gear with the given context and history. This is automatically called if executing a gear within a `switch` method; otherwise, it must be called manually for a top-level gear.\n\n        Args:\n            context (BaseModel): Input context for the gear. Must be a pydantic model. This is passed to the `template` method, which will construct the prompt for the LLM.\n            history (History): Chat history. This is passed to the LLM's `run` method.\n\n        Raises:\n            TypeError: If the `template` method does not return a string.\n            TypeError: If the `transform` method does not return a pydantic model.\n            TypeError: If the `switch` method does not return a Gear instance or None.\n\n        Returns:\n            BaseModel: The output context of the gear (result of `transform` method) if no gears are chained in the `switch` method; otherwise, the output context of the last gear in the chain.\n        \"\"\"\n        # Construct the template with the pydantic model\n        prompt = Template(self.template(context)).render(context=context)\n        if not isinstance(prompt, str):\n            raise TypeError(\"Template must return a string\")\n\n        # Call the model\n        logger.info(f\"Running model with prompt: {prompt}\")\n        response = await self.model.run(prompt, history, **kwargs)\n\n        # Transform the data from the response\n        response = self.transform(response, context)\n        # Verify that the structured data is a pydantic model\n        if not isinstance(response, BaseModel):\n            raise TypeError(\"Transform must return a pydantic model instance\")\n\n        # Load which other gear to run, if any\n        try:\n            child = self.switch(response)\n            if isinstance(child, Gear):\n                return await child.run(response, history, **kwargs)\n            elif not child:\n                logger.info(\"No child gear to run. Returning response.\")\n                return response\n            else:\n                raise TypeError(f\"Switch must return a Gear instance or None.\")\n\n        except NotImplementedError:\n            pass\n\n        # If there is no child, return the response\n        return response\n\n    @abstractmethod\n    def template(self, context: BaseModel) -&gt; str:\n\"\"\"Template for the LLM prompt. This is a jinja2 template that will be rendered with the given context.\n\n        Args:\n            context (BaseModel): Pydantic model instance that will be used to render the template.\n\n        Raises:\n            NotImplementedError: If the gear does not implement this method.\n\n        Returns:\n            str: Prompt template that will be rendered with the given context.\n        \"\"\"\n        raise NotImplementedError(\"Gear must implement prompt template\")\n\n    @abstractmethod\n    def transform(self, response: dict, context: BaseModel) -&gt; BaseModel:\n\"\"\"Transforms the response from the LLM into a pydantic model instance.\n\n        Args:\n            response (dict): Raw response from the LLM.\n            context (BaseModel): Input context for the gear. Must be a pydantic model.\n\n        Raises:\n            NotImplementedError: If the gear does not implement this method.\n\n        Returns:\n            BaseModel: New context for the gear. Must be a pydantic model instance.\n        \"\"\"\n        raise NotImplementedError(\"Gear must implement transform method\")\n\n    def switch(self, return_context: BaseModel) -&gt; Optional[\"Gear\"]:\n\"\"\"Method that determines which gear to run next. This method is called after the `transform` method with the returned context. If this method is not implemented, then the gear will return the response from the `transform` method.\n\n        Args:\n            return_context (BaseModel): Output context from the `transform` method.\n\n        Returns:\n            Optional[Gear]: The next gear to run (an instance). If None, then the gear will return the response from the `transform` method.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.__init__","title":"<code>__init__(model: BaseLLM)</code>","text":"<p>Initializes a Gear with a LLM model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseLLM</code> <p>The LLM model to use. Must be an instance of a class that inherits from BaseLLM.</p> required Source code in <code>gears/gear.py</code> <pre><code>def __init__(\n    self,\n    model: BaseLLM,\n):\n\"\"\"Initializes a Gear with a LLM model.\n\n    Args:\n        model (BaseLLM): The LLM model to use. Must be an instance of a class that inherits from BaseLLM.\n    \"\"\"\n    self.model = model\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.template","title":"<code>template(context: BaseModel) -&gt; str</code>  <code>abstractmethod</code>","text":"<p>Template for the LLM prompt. This is a jinja2 template that will be rendered with the given context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>BaseModel</code> <p>Pydantic model instance that will be used to render the template.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the gear does not implement this method.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Prompt template that will be rendered with the given context.</p> Source code in <code>gears/gear.py</code> <pre><code>@abstractmethod\ndef template(self, context: BaseModel) -&gt; str:\n\"\"\"Template for the LLM prompt. This is a jinja2 template that will be rendered with the given context.\n\n    Args:\n        context (BaseModel): Pydantic model instance that will be used to render the template.\n\n    Raises:\n        NotImplementedError: If the gear does not implement this method.\n\n    Returns:\n        str: Prompt template that will be rendered with the given context.\n    \"\"\"\n    raise NotImplementedError(\"Gear must implement prompt template\")\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.transform","title":"<code>transform(response: dict, context: BaseModel) -&gt; BaseModel</code>  <code>abstractmethod</code>","text":"<p>Transforms the response from the LLM into a pydantic model instance.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict</code> <p>Raw response from the LLM.</p> required <code>context</code> <code>BaseModel</code> <p>Input context for the gear. Must be a pydantic model.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the gear does not implement this method.</p> <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>BaseModel</code> <p>New context for the gear. Must be a pydantic model instance.</p> Source code in <code>gears/gear.py</code> <pre><code>@abstractmethod\ndef transform(self, response: dict, context: BaseModel) -&gt; BaseModel:\n\"\"\"Transforms the response from the LLM into a pydantic model instance.\n\n    Args:\n        response (dict): Raw response from the LLM.\n        context (BaseModel): Input context for the gear. Must be a pydantic model.\n\n    Raises:\n        NotImplementedError: If the gear does not implement this method.\n\n    Returns:\n        BaseModel: New context for the gear. Must be a pydantic model instance.\n    \"\"\"\n    raise NotImplementedError(\"Gear must implement transform method\")\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.switch","title":"<code>switch(return_context: BaseModel) -&gt; Optional[Gear]</code>","text":"<p>Method that determines which gear to run next. This method is called after the <code>transform</code> method with the returned context. If this method is not implemented, then the gear will return the response from the <code>transform</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>return_context</code> <code>BaseModel</code> <p>Output context from the <code>transform</code> method.</p> required <p>Returns:</p> Type Description <code>Optional[Gear]</code> <p>Optional[Gear]: The next gear to run (an instance). If None, then the gear will return the response from the <code>transform</code> method.</p> Source code in <code>gears/gear.py</code> <pre><code>def switch(self, return_context: BaseModel) -&gt; Optional[\"Gear\"]:\n\"\"\"Method that determines which gear to run next. This method is called after the `transform` method with the returned context. If this method is not implemented, then the gear will return the response from the `transform` method.\n\n    Args:\n        return_context (BaseModel): Output context from the `transform` method.\n\n    Returns:\n        Optional[Gear]: The next gear to run (an instance). If None, then the gear will return the response from the `transform` method.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/gears/#gears.gear.Gear.run","title":"<code>run(context: BaseModel, history: History, **kwargs: History) -&gt; BaseModel</code>  <code>async</code>","text":"<p>Runs the gear with the given context and history. This is automatically called if executing a gear within a <code>switch</code> method; otherwise, it must be called manually for a top-level gear.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>BaseModel</code> <p>Input context for the gear. Must be a pydantic model. This is passed to the <code>template</code> method, which will construct the prompt for the LLM.</p> required <code>history</code> <code>History</code> <p>Chat history. This is passed to the LLM's <code>run</code> method.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>template</code> method does not return a string.</p> <code>TypeError</code> <p>If the <code>transform</code> method does not return a pydantic model.</p> <code>TypeError</code> <p>If the <code>switch</code> method does not return a Gear instance or None.</p> <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>BaseModel</code> <p>The output context of the gear (result of <code>transform</code> method) if no gears are chained in the <code>switch</code> method; otherwise, the output context of the last gear in the chain.</p> Source code in <code>gears/gear.py</code> <pre><code>async def run(\n    self,\n    context: BaseModel,\n    history: History,\n    **kwargs,\n) -&gt; BaseModel:\n\"\"\"Runs the gear with the given context and history. This is automatically called if executing a gear within a `switch` method; otherwise, it must be called manually for a top-level gear.\n\n    Args:\n        context (BaseModel): Input context for the gear. Must be a pydantic model. This is passed to the `template` method, which will construct the prompt for the LLM.\n        history (History): Chat history. This is passed to the LLM's `run` method.\n\n    Raises:\n        TypeError: If the `template` method does not return a string.\n        TypeError: If the `transform` method does not return a pydantic model.\n        TypeError: If the `switch` method does not return a Gear instance or None.\n\n    Returns:\n        BaseModel: The output context of the gear (result of `transform` method) if no gears are chained in the `switch` method; otherwise, the output context of the last gear in the chain.\n    \"\"\"\n    # Construct the template with the pydantic model\n    prompt = Template(self.template(context)).render(context=context)\n    if not isinstance(prompt, str):\n        raise TypeError(\"Template must return a string\")\n\n    # Call the model\n    logger.info(f\"Running model with prompt: {prompt}\")\n    response = await self.model.run(prompt, history, **kwargs)\n\n    # Transform the data from the response\n    response = self.transform(response, context)\n    # Verify that the structured data is a pydantic model\n    if not isinstance(response, BaseModel):\n        raise TypeError(\"Transform must return a pydantic model instance\")\n\n    # Load which other gear to run, if any\n    try:\n        child = self.switch(response)\n        if isinstance(child, Gear):\n            return await child.run(response, history, **kwargs)\n        elif not child:\n            logger.info(\"No child gear to run. Returning response.\")\n            return response\n        else:\n            raise TypeError(f\"Switch must return a Gear instance or None.\")\n\n    except NotImplementedError:\n        pass\n\n    # If there is no child, return the response\n    return response\n</code></pre>"},{"location":"api/history/","title":"history","text":""},{"location":"api/history/#gears.History","title":"<code>gears.History</code>","text":"<p>This class is a wrapper around a list of messages. It is used to keep track of the conversation history with an LLM.</p> <p>A history is a list of messages. Each message has a role and content. The role is something like \"user\" or \"system\". The content is a string.</p>"},{"location":"api/history/#gears.history.History.cost","title":"<code>cost</code>  <code>property</code>","text":""},{"location":"api/history/#gears.history.History.__init__","title":"<code>__init__(system_message: str = None)</code>","text":"<p>Constructor for the History class.</p> <p>Parameters:</p> Name Type Description Default <code>system_message</code> <code>str</code> <p>System message for the LLM to use, if any. E.g., \"You are a helpful assistant.\" Defaults to None.</p> <code>None</code>"},{"location":"api/history/#gears.history.History.increment_cost","title":"<code>increment_cost(cost: float)</code>","text":"<p>Increments the cost of the history.</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>float</code> <p>Float representing the cost to increment by (in dollars)</p> required"},{"location":"api/history/#gears.history.History.add","title":"<code>add(message: Message)</code>","text":"<p>Adds a message to the history. Used in LLM run methods.</p>"},{"location":"api/history/#gears.history.History.__getitem__","title":"<code>__getitem__(index: int)</code>","text":"<p>Returns the message at the given index.</p>"},{"location":"api/history/#gears.history.History.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of messages in the history.</p>"},{"location":"api/history/#gears.history.History.__iter__","title":"<code>__iter__()</code>","text":"<p>An iterator over the messages in the history.</p> <p>Usage: <pre><code>h = History()\n\nfor message in h:\n    print(message)\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Any</code> <p>An iterator over the messages in the history.</p>"},{"location":"api/history/#gears.history.History.__str__","title":"<code>__str__()</code>","text":"<p>String representation of the history.</p>"},{"location":"api/history/#gears.history.Message","title":"<code>gears.history.Message</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This class is a wrapper around a message. It is a building block of a history.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the message. Something like \"user\" or \"system\".</p> <code>content</code> <code>str</code> <p>The content of the message. A string.</p> Source code in <code>gears/history.py</code> <pre><code>class Message(BaseModel, extra=\"allow\"):\n\"\"\"This class is a wrapper around a message. It is a building block of a history.\n\n    Attributes:\n        role (str): The role of the message. Something like \"user\" or \"system\".\n        content (str): The content of the message. A string.\n    \"\"\"\n\n    role: str = Field(\n        ...,\n        description=\"The role of the message. Something like 'user' or 'system'.\",\n    )\n    content: str = Field(\n        ..., description=\"The content of the message. A string.\"\n    )\n</code></pre>"},{"location":"api/llms/","title":"llms","text":""},{"location":"api/llms/#gears.llms.BaseLLM","title":"<code>gears.llms.BaseLLM</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>gears/llms/base.py</code> <pre><code>class BaseLLM(ABC):\n    @abstractmethod\n    async def run(\n        prompt: str,\n        history: History,\n        message_kwargs: dict = {},\n    ):\n        pass\n</code></pre>"},{"location":"api/llms/#gears.llms.base.BaseLLM.run","title":"<code>run(prompt: str, history: History, message_kwargs: dict = {})</code>  <code>abstractmethod</code> <code>async</code>","text":"Source code in <code>gears/llms/base.py</code> <pre><code>@abstractmethod\nasync def run(\n    prompt: str,\n    history: History,\n    message_kwargs: dict = {},\n):\n    pass\n</code></pre>"},{"location":"api/llms/#gears.llms.OpenAIChat","title":"<code>gears.llms.OpenAIChat</code>","text":"<p>             Bases: <code>BaseLLM</code></p>"},{"location":"api/llms/#gears.llms.oai.OpenAIChat.__init__","title":"<code>__init__(model: str = 'gpt-3.5-turbo', max_retries: int = 3, **kwargs: int)</code>","text":"<p>Creates an OpenAI chat API wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>String representing the name of the model. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo'</code> <code>max_retries</code> <code>int</code> <p>Max number of retries for calling the OpenAI API. Defaults to 3.</p> <code>3</code> <code>**kwargs</code> <p>Any additional kwargs to pass to the OpenAI chat API.</p> <code>{}</code>"},{"location":"api/llms/#gears.llms.oai.OpenAIChat.run","title":"<code>run(prompt: str, history: History, **message_kwargs: Any) -&gt; Any</code>  <code>async</code>","text":"<p>Calls the OpenAI chat API with the given prompt and chat history. Also adds the response to the chat history and increments the cost.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to send to the OpenAI chat API.</p> required <code>history</code> <code>History</code> <p>History of the chat so far.</p> required <code>**message_kwargs</code> <code>Any</code> <p>Any additional kwargs to pass to the message.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Raw response from the OpenAI chat API.</p>"},{"location":"api/llms/#gears.llms.AzureOpenAIChat","title":"<code>gears.llms.AzureOpenAIChat</code>","text":"<p>             Bases: <code>OpenAIChat</code></p>"},{"location":"api/llms/#gears.llms.oai.AzureOpenAIChat.__init__","title":"<code>__init__(model: str = 'gpt-35-turbo', deployment_id: str = 'gpt-35-turbo', max_retries: int = 3, **kwargs: int)</code>","text":"<p>Creates an Azure OpenAI chat API wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>String representing the name of the model. Defaults to \"gpt-3.5-turbo\". Used only to determine pricing.</p> <code>'gpt-35-turbo'</code> <code>deployment_id</code> <code>str</code> <p>String representing the deployment ID of the model based on your Azure Deployment. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-35-turbo'</code> <code>max_retries</code> <code>int</code> <p>Max number of retries for calling the OpenAI API. Defaults to 3.</p> <code>3</code> <code>**kwargs</code> <p>Any additional kwargs to pass to the chat API.</p> <code>{}</code>"},{"location":"api/llms/#gears.llms.oai.AzureOpenAIChat.run","title":"<code>run(prompt: str, history: History, **message_kwargs: Any) -&gt; Any</code>  <code>async</code>","text":"<p>Calls the Azure OpenAI chat API with the given prompt and chat history. Also adds the response to the chat history and increments the cost.</p> <p>Uses the <code>deployment_id</code> to call the relevant Azure OpenAI chat API.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to send to the OpenAI chat API.</p> required <code>history</code> <code>History</code> <p>History of the chat so far.</p> required <code>**message_kwargs</code> <code>Any</code> <p>Any additional kwargs to pass to the message.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Raw response from the OpenAI chat API.</p>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#gears.utils.extract_first_json","title":"<code>gears.utils.extract_first_json(text: str) -&gt; Union[Dict, List]</code>","text":"<p>Extracts the first JSON object or array in a string representing the output of an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The string to parse.</p> required <p>Returns:</p> Type Description <code>Union[Dict, List]</code> <p>Union[Dict, List]: The first JSON object or array.</p> Source code in <code>gears/utils.py</code> <pre><code>def extract_first_json(text: str) -&gt; Union[Dict, List]:\n\"\"\"\n    Extracts the first JSON object or array in a string representing the output of an LLM.\n\n    Args:\n        text (str): The string to parse.\n\n    Returns:\n        Union[Dict, List]: The first JSON object or array.\n    \"\"\"\n    try:\n        return next(extract_json(text))\n    except StopIteration:\n        raise StopIteration(f\"No JSON found in text: {text}\")\n</code></pre>"},{"location":"api/utils/#gears.utils.extract_json","title":"<code>gears.utils.extract_json(text, decoder = json.JSONDecoder()) -&gt; Generator[Union[Dict, List], None, None]</code>","text":"<p>Generates all JSON objects or arrays in a string representing the output of an LLM. Copied from ChatGPT.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The string to parse.</p> required <p>Returns:</p> Type Description <code>Generator[Union[Dict, List], None, None]</code> <p>Generator[Union[Dict, List], None, None]: A generator of JSON objects or arrays.</p> Source code in <code>gears/utils.py</code> <pre><code>def extract_json(\n    text, decoder=json.JSONDecoder()\n) -&gt; Generator[Union[Dict, List], None, None]:\n\"\"\"\n    Generates all JSON objects or arrays in a string representing the output of an LLM. Copied from ChatGPT.\n\n    Args:\n        text (str): The string to parse.\n\n    Returns:\n        Generator[Union[Dict, List], None, None]: A generator of JSON objects or arrays.\n    \"\"\"\n    pos = 0\n    while True:\n        match = text.find(\"{\", pos)\n        match2 = text.find(\"[\", pos)\n        # Find the earliest opening bracket\n        if (match != -1 and match &lt; match2) or match2 == -1:\n            start_pos = match\n        else:\n            start_pos = match2\n\n        if start_pos == -1:\n            break\n        try:\n            result, index = decoder.raw_decode(text[start_pos:])\n            yield result\n            pos = start_pos + index\n        except json.JSONDecodeError:\n            pos = start_pos + 1\n</code></pre>"},{"location":"examples/advanced/","title":"Personalized Vacation Planner (Advanced Control Flow)","text":"<p>Sometimes we will want to generate a \"tree\" of calls to an LLM, where the next call depends on the output of the previous call. For example, suppose we want to generate a structured vacation itinerary for a user that includes activities that are dependent on the climate.</p>"},{"location":"examples/advanced/#0-high-level-overview","title":"0. High-Level Overview","text":"<p>In this application, we'll use LLMs to:</p> <ul> <li>Based on a home location, pick a place to travel and season to travel there</li> <li>Select 2 activities to do at the destination, including outdoor and indoor activities (weather permitting)</li> <li>Summarize the activities into a travel itinerary</li> </ul>"},{"location":"examples/advanced/#1-create-a-context","title":"1. Create a Context","text":"<p>We'll set up a Pydantic model with information we'll extract from the LLM:</p> <pre><code>from gears import Gear, History, OpenAIChat\nfrom pydantic import BaseModel\nfrom typing import Any\n\nimport asyncio\n\nclass Suggestion(BaseModel):\n    home: str\n    destination: str = None\n    season: str = None\n    activities: list = None\n    itinerary: str = None\n</code></pre>"},{"location":"examples/advanced/#2-create-a-gear-to-select-a-destinationseason-and-outdoorindoor-activities","title":"2. Create a Gear to Select a Destination/Season and Outdoor/Indoor Activities","text":"<p>We'll create a <code>Gear</code> that takes a home location and generates a destination and season to travel there:</p> <pre><code>from gears.utils import extract_first_json\n\nclass DestinationSelection(Gear):\n    def template(self, context: Suggestion):\n        return \"Suggest a city within a 4-5 hour flight that someone who lives in {{ context.home }} can travel to for a vacation. Pick a season that is best to travel to this destination in. Output the destination and season as a JSON with keys `destination` and `season` and values equal to the destination and season, respectively.\"\n\n    def transform(self, response: dict, context: Suggestion):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        # Get JSON from reply and execute it\n        answer = extract_first_json(reply, context)\n        return Suggestion(home=context.home, destination=answer[\"destination\"], season=answer[\"season\"])\n\n    def switch(self, context: Suggestion):\n        return IndoorOrOutdoor(OpenAIChat(\"gpt-3.5-turbo\"))\n\nclass IndoorOrOutdoor(Gear):\n    def template(self, context: Suggestion):\n        return \"Based on the expected weather at the destination in the {{ context.season }} season, can the person do outdoor activities? Output your answer as a JSON with key `outdoor` and value equal to `yes` or `no`, respectively.\"\n\n    def transform(self, response: dict, context: Suggestion):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        # Get JSON from reply and execute it\n        answer = extract_first_json(reply, context)\n        return Suggestion(home=context.home, destination=answer[\"destination\"], season=answer[\"season\"], outdoor=answer[\"outdoor\"])\n\n    def switch(self, context: Suggestion):\n        if context.outdoor == \"yes\":\n            return OutdoorActivitySelection(OpenAIChat(\"gpt-3.5-turbo\"))\n        else:\n            return IndoorActivitySelection(OpenAIChat(\"gpt-3.5-turbo\"))\n</code></pre>"},{"location":"examples/advanced/#3-create-gears-to-select-the-activities","title":"3. Create Gears to Select the Activities","text":"<p>Now, we'll ask the LLM to select activities to do at the destination:</p> <pre><code>class OutdoorActivitySelection(Gear):\n    def template(self, context: Suggestion):\n        return \"Suggest a popular outdoor activity to do in the city of {{ context.destination }} during the season of {{ context.season }}.\"\n\n    def transform(self, response: dict, context: Suggestion):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        activities = [reply]\n        return Suggestion(home=context.home, destination=context.destination, season=context.season, activities=activities, outdoor=context.outdoor)\n\n    def switch(self, context: Suggestion):\n        return IndoorOrOutdoor(OpenAIChat(\"gpt-3.5-turbo\"))\n\nclass IndoorActivitySelection(Gear):\n    def template(self, context: Suggestion):\n        # If there is already an indoor activity, prompt for a different one\n        if context.activities:\n            return \"Suggest a different popular indoor activity to do in the city of {{ context.destination }} during the season of {{ context.season }}.\"\n        else:\n            return \"Suggest a popular indoor activity to do in the city of {{ context.destination }} during the season of {{ context.season }}.\"\n\n    def transform(self, response: dict, context: Suggestion):\n       reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        activities = context.activities + [reply] if context.activities else [reply]\n        return Suggestion(home=context.home, destination=context.destination, season=context.season, activities=activities, outdoor=context.outdoor)\n\n    def switch(self, context: Suggestion):\n        if context.outdoor != \"yes\":\n            # Need another indoor activity\n            return IndoorActivitySelection(OpenAIChat(\"gpt-3.5-turbo\"))\n        else:\n            # Already have 2 activities. Go to summary\n            return SummarizeItinerary(OpenAIChat(\"gpt-3.5-turbo\"))\n</code></pre> <p>Note that we have a special case in the <code>IndoorActivitySelection</code> gear: if the LLM recommended not to do outdoor activities at all, then we ask for 2 indoor activities. Otherwise, we ask for 1 indoor activity and 1 outdoor activity.</p>"},{"location":"examples/advanced/#4-create-a-gear-to-summarize-the-itinerary","title":"4. Create a Gear to Summarize the Itinerary","text":"<p>Finally, we'll ask the LLM to summarize the itinerary:</p> <pre><code>class SummarizeItinerary(Gear):\n    def template(self, context: Suggestion):\n        return \"Summarize your suggested activities: {{ context.activities }} into a short personalized vacation itinerary for someone who lives in {{ context.home }} to travel to {{ context.destination }} during the {{ context.season }} season.\"\n\n    def transform(self, response: dict, context: Suggestion):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        return Suggestion(home=context.home, destination=context.destination, season=context.season, activities=activities, outdoor=context.outdoor, itinerary=reply)\n</code></pre> <p>The <code>switch</code> method is not implemented, so the workflow will end after this gear.</p>"},{"location":"examples/advanced/#5-run-the-workflow","title":"5. Run the Workflow","text":"<p>TODO</p>"},{"location":"examples/customllm/","title":"Creating a Custom LLM for Gears","text":"<p>Gears supports <code>openai</code>'s chat models (as well as the Azure version) out of the box, but you can easily add your own LLMs by subclassing <code>BaseLLM</code> and following the steps below.</p> <p>To create a custom LLM, you should subclass <code>BaseLLM</code>:</p> <pre><code>class BaseLLM(ABC):\n    @abstractmethod\n    async def run(\n        prompt: str,\n        history: History,\n        message_kwargs: dict = {},\n    ):\n        pass\n</code></pre> <p>The <code>run</code> method should take in a prompt, a <code>History</code> object, and any other keyword arguments you want to pass to the LLM. The <code>run</code> method should return a response from the LLM, which will be passed to the <code>transform</code> method of the <code>Gear</code> that called the LLM. The <code>run</code> method should also update the <code>History</code> object with the request and response data, and update the history's cost.</p> <p>Here is the <code>run</code> method for <code>OpenAIChat</code>, which constructs a chat history and calls the OpenAI chat API:</p> <pre><code>async def run(\n        self,\n        prompt: str,\n        history: History,\n        **message_kwargs: Any,\n    ) -&gt; Any:\n        # Construct chat history\n        curr_message = Message(role=\"user\", content=prompt, **message_kwargs)\n        history.add(curr_message)\n        try:\n            messages = [m.model_dump() for m in history]\n        except AttributeError:\n            messages = [m.dict() for m in history]\n        request = {\n            \"model\": self.model,\n            \"messages\": messages,\n            **self.api_kwargs,\n        }\n        response = await self.chat_api_call(request) # (1)!\n        try:\n            returned_message = response[\"choices\"][0][\"message\"][\"content\"]\n            history.add(\n                Message(\n                    role=response[\"choices\"][0][\"message\"][\"role\"],\n                    content=returned_message,\n                )\n            ) # (2)!\n        except KeyError:\n            logger.error(\n                f\"Could not find message and/or role in response: {response}\"\n            )\n\n        # Increment cost\n        try:\n            prompt_tokens = response[\"usage\"][\"prompt_tokens\"]\n            completion_tokens = response[\"usage\"][\"completion_tokens\"]\n\n            cost = (\n                OPENAI_PRICING_MAP[self.model_base][\"prompt_tokens\"]\n                * prompt_tokens\n                + OPENAI_PRICING_MAP[self.model_base][\"completion_tokens\"]\n                * completion_tokens\n            )\n            history.increment_cost(cost) # (3)!\n        except KeyError:\n            logger.error(\n                f\"Could not find pricing for model {self.model}. Not incrementing cost.\"\n            )\n\n        return response\n</code></pre> <ol> <li><code>chat_api_call</code> is a helper method that calls the OpenAI chat API with the request constructed above.</li> <li><code>history.add</code> is a helper method that adds a message to the history. You must add messages to the history to be used in downstream Gears.</li> <li><code>history.increment_cost</code> is a helper method that increments the cost of the history. This way, after a workflow is run, you can see how much it cost to run the workflow.</li> </ol>"},{"location":"examples/simple/","title":"Text to Executable SQL (Simple Control Flow)","text":"<p>While many natural language to SQL systems these days can generate compilable SQL, it's hard for them to generate executable SQL. For example, the following query is not executable if the <code>users</code> table does not have a <code>name</code> column:</p> <pre><code>SELECT * FROM users WHERE name = \"Alice\"\n</code></pre> <p>One of the areas where <code>gears</code> shines is the ability to validate LLM output and dynamically decide what to do. In this tutorial, we will build a simple <code>Gear</code> that generates executable SQL from a natural language query.</p>"},{"location":"examples/simple/#0-downloads","title":"0. Downloads","text":"<p>Assuming you've already configured your <code>openai</code> keys, download the Python libraries we'll be using in this tutorial:</p> <pre><code>pip install duckdb\n</code></pre> <p>We'll be using the NYC Taxicab dataset, described in this DuckDB blog post. Download it using <code>wget</code> like so:</p> <pre><code>wget https://github.com/cwida/duckdb-data/releases/download/v1.0/taxi_2019_04.parquet\n</code></pre>"},{"location":"examples/simple/#1-create-a-context","title":"1. Create a Context","text":"<p>We'll set up a <code>Context</code>:</p> <pre><code>from gears import Gear, History, OpenAIChat\nfrom pydantic import BaseModel\nfrom typing import Any\n\nimport asyncio\nimport duckdb\n\nclass SQLContext(BaseModel):\n    nlquery: str\n    sql: str = None\n    exception: str = None\n    result: Any = None\n\ntable_statistics = duckdb.query(\"PRAGMA show_tables_expanded;\").fetchdf()\nsystem_prompt = f\"Here's some statistics about my database:\\n{table_statistics.to_string(index=False)}\"\n</code></pre>"},{"location":"examples/simple/#2-create-a-gear","title":"2. Create a Gear","text":"<p>We'll create a <code>Gear</code> that takes a natural language query and generates executable SQL:</p> <pre><code>from gears.utils import extract_first_json\n\nclass SQLGear(Gear):\n    def template(self, context: SQLContext):\n        if context.exception:\n            return \"Your query failed to execute with the following error: {{ context.exception }}\\n\\nPlease try again. Output the SQL as a JSON with key `sql` and value equal to the SQL query for me to run.\"\n\n        else:\n            return \"Translate the following query into SQL: {{ context.nlquery }}\\n\\nMake sure the SQL is executable. Output the SQL as a JSON with key `sql` and value equal to the SQL query for me to run.\"\n\n    def transform(self, response: dict, context: SQLContext):\n        reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\n        # Get JSON from reply and execute it\n        try:\n            sql = extract_first_json(reply, context) # (1)!\n\n            # Execute SQL\n            result = duckdb.query(sql).fetchall()\n            return SQLContext(nlquery=context.nlquery, sql=sql, result=result)\n\n        except Exception as e:\n            return SQLContext(nlquery=context.nlquery, exception=str(e))\n\n    def switch(self, context: SQLContext):\n        if context.exception is not None:\n            return SQLGear(OpenAIChat(\"gpt-3.5-turbo\"))\n        else:\n            return None\n</code></pre> <ol> <li><code>extract_first_json</code> is a helper function that extracts the first JSON from a string. You can write your own parser here if you'd like.</li> </ol>"},{"location":"examples/simple/#3-run-the-gear","title":"3. Run the Gear","text":"<p>We'll run the gear like so:</p> <pre><code>async def main():\n    context = SQLContext(nlquery=\"How many trips were taken in April 2019?\")\n    history = History(system_message=system_prompt)\n    llm = OpenAIChat(\"gpt-3.5-turbo\")\n\n    context = await SQLGear(llm).run(context, history)\n    print(f\"SQL query: {context.sql}\")\n    print(f\"SQL query result: {context.result}\")\n    print(f\"Cost of query: {history.cost}\")\n    print(history)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This will run the gear until it returns <code>None</code> from <code>switch</code>, denoting a valid SQL query result. The output should look something like this:</p> <pre><code>TODO\n</code></pre>"},{"location":"examples/simple/#extra-notes","title":"Extra Notes","text":"<p><code>gears</code> is not a full-fledged LLM guardrails library, nor does it intend to be. It is just a lightweight way to add some validation criteria to your LLMs by using control flow. If you want to use a full-fledged LLM guardrails library, I recommend you check out Guardrails, coincidentally written by another person named Shreya.</p>"}]}